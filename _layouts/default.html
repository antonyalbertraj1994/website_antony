<!-- <!DOCTYPE html>
<html lang="{{ site.lang | default: "en-US" }}">
  <head>

    {% if site.google_analytics %}
      <script async src="https://www.googletagmanager.com/gtag/js?id={{ site.google_analytics }}"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', '{{ site.google_analytics }}');
      </script>
    {% endif %}
    <meta charset="UTF-8">

{% seo %}
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="{{ '/assets/css/style.css?v=' | append: site.github.build_revision | relative_url }}">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <!-- <h1 class="project-name">{{ page.title | default: site.title | default: site.github.repository_name }}</h1> -->
  





<html xmlns:og="http://opengraphprotocol.org/schema/" xmlns:fb="http://www.facebook.com/2008/fbml" lang="en-US" class="yui3-js-enabled js flexbox canvas canvastext webgl no-touch hashchange history draganddrop rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms no-csstransforms3d csstransitions video audio svg inlinesvg svgclippaths wf-proximanova-n3-active wf-proximanova-n7-active wf-futurapt-n4-active wf-futurapt-n3-active wf-proximanova-n4-active wf-proximanova-i3-active wf-proximanova-i7-active wf-futurapt-n7-active wf-futurapt-i4-active wf-futurapt-i7-active wf-futurapt-i3-active wf-active" style="" id="yui_3_17_2_1_1618546122048_93"><div id="yui3-css-stamp" style="position: absolute !important; visibility: hidden !important"></div><head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="viewport" content="width=device-width,initial-scale=1">

  <!-- This is Squarespace. --><!-- figlab -->
<base href="">
<meta charset="utf-8">
<title>Future Interfaces Group</title>
<link rel="shortcut icon" type="image/x-icon" href="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447777266561-EZ4QK4QATHSOPC3WKRU5/ke17ZwdGBToddI8pDm48kJycfsYb1urLU93EpFqOTQmoCXeSvxnTEQmG4uwOsdIceAoHiyRoc52GMN5_2H8Wp7zww8OjRrqjaM7_0x6HDLp42EP6IAa5vAmscK3sHI4MkNL5tmfZ3otlI9yi1IzH2Q/favicon.ico">
<link rel="canonical" href="http://www.figlab.com">
<meta property="og:site_name" content="Future Interfaces Group">
<meta property="og:title" content="Future Interfaces Group">
<meta property="og:url" content="http://www.figlab.com">
<meta property="og:type" content="website">
<meta property="og:image" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1500w">
<meta property="og:image:width" content="805">
<meta property="og:image:height" content="300">
<meta itemprop="name" content="Future Interfaces Group">
<meta itemprop="url" content="http://www.figlab.com">
<meta itemprop="thumbnailUrl" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1500w">
<link rel="image_src" href="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1500w">
<meta itemprop="image" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1500w">
<meta name="twitter:title" content="Future Interfaces Group">
<meta name="twitter:image" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1500w">
<meta name="twitter:url" content="http://www.figlab.com">
<meta name="twitter:card" content="summary">
<meta name="description" content="">
<link rel="preconnect" href="https://images.squarespace-cdn.com">
<script type="text/javascript" src="//use.typekit.net/ik/kkckWeTqkB_ngUSQIHNAANeBtj28DfHCxD7Yf90JEYvfe7tffFHN4UJLFRbh52jhWD9hFeJuwQMaZQsKw26a52boF29kZAIXjyT0HKoc-AiCjAJ0SaBujW48Sagyjh90jhNlOeZTZhUyjKoRdhXCZc81deBKO1FUiABkZWF3jAF8OcFzdPUCdhFydeyzSabCiaiaOcZTZhUyjKoRdhXCiaiaOcZTZhUyjKoDSWmyScmDSeBRZPoRdhXK2YgkdayTdAIldcNhjPJ4Z1mXiW4yOWgXH6qJtKGbMg62JMJ7fbKzMsMMeMb6MKG4fJCgIMMjgkMfH6qJtkGbMg6FJMJ7fbKwMsMMegI6MKG4fJZmIMIjMkMfH6qJyB9bMs6IJMJ7fbKgmsMgeMS6MKG4fJFmIMIj2PMfH6qJym9bMs65JMJ7fbKfmsMgegI6MTMganDQob9.js"></script>
<style type="text/css">@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/56b0cd/00000000000000007735957d/30/l?subset_id=2&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/56b0cd/00000000000000007735957d/30/d?subset_id=2&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/56b0cd/00000000000000007735957d/30/a?subset_id=2&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;font-display:auto;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/d45b9a/000000000000000077359577/30/l?subset_id=2&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/d45b9a/000000000000000077359577/30/d?subset_id=2&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/d45b9a/000000000000000077359577/30/a?subset_id=2&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;font-display:auto;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/98e3f6/000000000000000077359562/30/l?subset_id=2&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/98e3f6/000000000000000077359562/30/d?subset_id=2&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/98e3f6/000000000000000077359562/30/a?subset_id=2&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;font-display:auto;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/fcae55/000000000000000077359580/30/l?subset_id=2&fvd=i3&v=3) format("woff2"),url(https://use.typekit.net/af/fcae55/000000000000000077359580/30/d?subset_id=2&fvd=i3&v=3) format("woff"),url(https://use.typekit.net/af/fcae55/000000000000000077359580/30/a?subset_id=2&fvd=i3&v=3) format("opentype");font-weight:300;font-style:italic;font-display:auto;}@font-face{font-family:proxima-nova;src:url(https://use.typekit.net/af/624cab/000000000000000077359558/30/l?subset_id=2&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/624cab/000000000000000077359558/30/d?subset_id=2&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/624cab/000000000000000077359558/30/a?subset_id=2&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;font-display:auto;}@font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/ae4f6c/000000000000000000010096/27/l?subset_id=2&fvd=n3&v=3) format("woff2"),url(https://use.typekit.net/af/ae4f6c/000000000000000000010096/27/d?subset_id=2&fvd=n3&v=3) format("woff"),url(https://use.typekit.net/af/ae4f6c/000000000000000000010096/27/a?subset_id=2&fvd=n3&v=3) format("opentype");font-weight:300;font-style:normal;font-display:auto;}@font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9b05f3/000000000000000000013365/27/l?subset_id=2&fvd=n4&v=3) format("woff2"),url(https://use.typekit.net/af/9b05f3/000000000000000000013365/27/d?subset_id=2&fvd=n4&v=3) format("woff"),url(https://use.typekit.net/af/9b05f3/000000000000000000013365/27/a?subset_id=2&fvd=n4&v=3) format("opentype");font-weight:400;font-style:normal;font-display:auto;}@font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/309dfe/000000000000000000010091/27/l?subset_id=2&fvd=n7&v=3) format("woff2"),url(https://use.typekit.net/af/309dfe/000000000000000000010091/27/d?subset_id=2&fvd=n7&v=3) format("woff"),url(https://use.typekit.net/af/309dfe/000000000000000000010091/27/a?subset_id=2&fvd=n7&v=3) format("opentype");font-weight:700;font-style:normal;font-display:auto;}@font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/849347/000000000000000000010093/27/l?subset_id=2&fvd=i3&v=3) format("woff2"),url(https://use.typekit.net/af/849347/000000000000000000010093/27/d?subset_id=2&fvd=i3&v=3) format("woff"),url(https://use.typekit.net/af/849347/000000000000000000010093/27/a?subset_id=2&fvd=i3&v=3) format("opentype");font-weight:300;font-style:italic;font-display:auto;}@font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/cf3e4e/000000000000000000010095/27/l?subset_id=2&fvd=i4&v=3) format("woff2"),url(https://use.typekit.net/af/cf3e4e/000000000000000000010095/27/d?subset_id=2&fvd=i4&v=3) format("woff"),url(https://use.typekit.net/af/cf3e4e/000000000000000000010095/27/a?subset_id=2&fvd=i4&v=3) format("opentype");font-weight:400;font-style:italic;font-display:auto;}@font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/eb729a/000000000000000000010092/27/l?subset_id=2&fvd=i7&v=3) format("woff2"),url(https://use.typekit.net/af/eb729a/000000000000000000010092/27/d?subset_id=2&fvd=i7&v=3) format("woff"),url(https://use.typekit.net/af/eb729a/000000000000000000010092/27/a?subset_id=2&fvd=i7&v=3) format("opentype");font-weight:700;font-style:italic;font-display:auto;}</style><script type="text/javascript">try{Typekit.load();}catch(e){}</script>
<script type="text/javascript">SQUARESPACE_ROLLUPS = {};</script>
<script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/moment-js-vendor-26ddeab7fa5f90b6c8cb3-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-moment_js_vendor');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/moment-js-vendor-26ddeab7fa5f90b6c8cb3-min.en-US.js"></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/cldr-resource-pack-7d6dc599f0e9e5882dcca-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-cldr_resource_pack');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/cldr-resource-pack-7d6dc599f0e9e5882dcca-min.en-US.js"></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/common-vendors-stable-7eaa020043cd8980b39ad-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common_vendors_stable');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/common-vendors-stable-7eaa020043cd8980b39ad-min.en-US.js"></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/common-vendors-b898d527b7159c7c8cdb5-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common_vendors');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/common-vendors-b898d527b7159c7c8cdb5-min.en-US.js"></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/common-903f41ae2a0e6c53ce926-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/common-903f41ae2a0e6c53ce926-min.en-US.js"></script><script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//assets.squarespace.com/universal/scripts-compressed/performance-a7b609eccb65e08a74111-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-performance');</script>
<script crossorigin="anonymous" src="//assets.squarespace.com/universal/scripts-compressed/performance-a7b609eccb65e08a74111-min.en-US.js" defer=""></script><script data-name="static-context">Static = window.Static || {}; Static.SQUARESPACE_CONTEXT = {"facebookAppId":"314192535267336","facebookApiVersion":"v6.0","rollups":{"squarespace-announcement-bar":{"js":"//assets.squarespace.com/universal/scripts-compressed/announcement-bar-ba74b54085e3bea818d5d-min.en-US.js"},"squarespace-audio-player":{"css":"//assets.squarespace.com/universal/styles-compressed/audio-player-7273d8fcec67906942b35-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/audio-player-1e420a3cff958891ec431-min.en-US.js"},"squarespace-blog-collection-list":{"css":"//assets.squarespace.com/universal/styles-compressed/blog-collection-list-3d55c64c25996c7633fc2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/blog-collection-list-41fb020c05858537c749a-min.en-US.js"},"squarespace-calendar-block-renderer":{"css":"//assets.squarespace.com/universal/styles-compressed/calendar-block-renderer-9433b542cea7a0e43eaa9-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/calendar-block-renderer-27ffa5993be80ff87fa7d-min.en-US.js"},"squarespace-chartjs-helpers":{"css":"//assets.squarespace.com/universal/styles-compressed/chartjs-helpers-58ae73137091cd0a61360-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/chartjs-helpers-90a05700972d4d6f84fa8-min.en-US.js"},"squarespace-comments":{"css":"//assets.squarespace.com/universal/styles-compressed/comments-eeb99f32a31032af774cb-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/comments-940689f6a014e8296baad-min.en-US.js"},"squarespace-commerce-cart":{"js":"//assets.squarespace.com/universal/scripts-compressed/commerce-cart-6e6270aa193fe5bb5bc57-min.en-US.js"},"squarespace-dialog":{"css":"//assets.squarespace.com/universal/styles-compressed/dialog-5c8a844a52d51995a3a8a-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/dialog-1866f40d364eaf7da0d3c-min.en-US.js"},"squarespace-events-collection":{"css":"//assets.squarespace.com/universal/styles-compressed/events-collection-9433b542cea7a0e43eaa9-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/events-collection-2a7217aba399bb232d787-min.en-US.js"},"squarespace-form-rendering-utils":{"js":"//assets.squarespace.com/universal/scripts-compressed/form-rendering-utils-43fbcdbb7ffeb4106ffdd-min.en-US.js"},"squarespace-forms":{"css":"//assets.squarespace.com/universal/styles-compressed/forms-1cc007b21ede0b73086c9-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/forms-5a05374f1bc11b2d3996e-min.en-US.js"},"squarespace-gallery-collection-list":{"css":"//assets.squarespace.com/universal/styles-compressed/gallery-collection-list-3d55c64c25996c7633fc2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/gallery-collection-list-cd5d8e548068a3a2586f7-min.en-US.js"},"squarespace-image-zoom":{"css":"//assets.squarespace.com/universal/styles-compressed/image-zoom-4a2adec1e747da2a6c89f-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/image-zoom-fa19933bfc5ac12f6e17f-min.en-US.js"},"squarespace-pinterest":{"css":"//assets.squarespace.com/universal/styles-compressed/pinterest-3d55c64c25996c7633fc2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/pinterest-04469c1c9eb87264ddea8-min.en-US.js"},"squarespace-popup-overlay":{"css":"//assets.squarespace.com/universal/styles-compressed/popup-overlay-e4ea05bd2ae9c1568e432-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/popup-overlay-59673c2fb17d60a80690b-min.en-US.js"},"squarespace-product-quick-view":{"css":"//assets.squarespace.com/universal/styles-compressed/product-quick-view-845801b442674594b4fd4-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/product-quick-view-e7158afa0ea30704daa36-min.en-US.js"},"squarespace-products-collection-item-v2":{"css":"//assets.squarespace.com/universal/styles-compressed/products-collection-item-v2-4a2adec1e747da2a6c89f-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/products-collection-item-v2-5f8dba4ad87c45ab68d86-min.en-US.js"},"squarespace-products-collection-list-v2":{"css":"//assets.squarespace.com/universal/styles-compressed/products-collection-list-v2-4a2adec1e747da2a6c89f-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/products-collection-list-v2-1836a36b2936989782757-min.en-US.js"},"squarespace-search-page":{"css":"//assets.squarespace.com/universal/styles-compressed/search-page-568ad8f2a40e76c0175c8-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/search-page-544005214322b37622637-min.en-US.js"},"squarespace-search-preview":{"js":"//assets.squarespace.com/universal/scripts-compressed/search-preview-bf4c309c44a0b79840b36-min.en-US.js"},"squarespace-share-buttons":{"js":"//assets.squarespace.com/universal/scripts-compressed/share-buttons-c4214afe6907e5aa1c4cc-min.en-US.js"},"squarespace-simple-liking":{"css":"//assets.squarespace.com/universal/styles-compressed/simple-liking-99bb613caaed2bf3e1efa-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/simple-liking-86a6810967083b1f49e97-min.en-US.js"},"squarespace-social-buttons":{"css":"//assets.squarespace.com/universal/styles-compressed/social-buttons-8d9d49ea1937426a8305f-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/social-buttons-281f17166912eca30a62f-min.en-US.js"},"squarespace-tourdates":{"css":"//assets.squarespace.com/universal/styles-compressed/tourdates-3d55c64c25996c7633fc2-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/tourdates-3f93661f51646d1bda121-min.en-US.js"},"squarespace-website-overlays-manager":{"css":"//assets.squarespace.com/universal/styles-compressed/website-overlays-manager-5c2f030f6ee94f066dc3d-min.en-US.css","js":"//assets.squarespace.com/universal/scripts-compressed/website-overlays-manager-cefb2d74acd0821459f72-min.en-US.js"}},"pageType":1,"website":{"id":"564b5107e4b087849452ea4b","identifier":"figlab","websiteType":1,"contentModifiedOn":1610557715131,"cloneable":false,"hasBeenCloneable":false,"developerMode":true,"siteStatus":{},"language":"en-US","timeZone":"America/New_York","machineTimeZoneOffset":-14400000,"timeZoneOffset":-14400000,"timeZoneAbbr":"EDT","siteTitle":"Future Interfaces Group","fullSiteTitle":"Future Interfaces Group","siteTagLine":"","siteDescription":"","logoImageId":"56742da94bf118e324766841","shareButtonOptions":{"7":true,"3":true,"2":true,"4":true,"8":true,"1":true,"6":true},"logoImageUrl":"//static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1610557715131/","authenticUrl":"http://www.figlab.com","internalUrl":"http://figlab.squarespace.com","baseUrl":"http://www.figlab.com","primaryDomain":"www.figlab.com","typekitId":"","statsMigrated":false,"imageMetadataProcessingEnabled":false,"screenshotId":"a93b0601","showOwnerLogin":false},"websiteSettings":{"id":"564b5107e4b087849452ea4e","websiteId":"564b5107e4b087849452ea4b","type":"Personal","subjects":[],"country":"US","state":"PA","simpleLikingEnabled":true,"mobileInfoBarSettings":{"isContactEmailEnabled":false,"isContactPhoneNumberEnabled":false,"isLocationEnabled":false,"isBusinessHoursEnabled":false},"commentLikesAllowed":true,"commentAnonAllowed":true,"commentThreaded":true,"commentApprovalRequired":false,"commentAvatarsOn":true,"commentSortType":2,"commentFlagThreshold":0,"commentFlagsAllowed":true,"commentEnableByDefault":true,"commentDisableAfterDaysDefault":0,"disqusShortname":"","commentsEnabled":false,"contactPhoneNumber":"","storeSettings":{"returnPolicy":null,"termsOfService":null,"privacyPolicy":null,"expressCheckout":false,"continueShoppingLinkUrl":"/","useLightCart":false,"showNoteField":false,"shippingCountryDefaultValue":"US","billToShippingDefaultValue":false,"showShippingPhoneNumber":true,"isShippingPhoneRequired":false,"showBillingPhoneNumber":true,"isBillingPhoneRequired":false,"currenciesSupported":["USD","CAD","GBP","AUD","EUR","CHF"],"defaultCurrency":"USD","selectedCurrency":"USD","measurementStandard":1,"showCustomCheckoutForm":false,"enableMailingListOptInByDefault":true,"sameAsRetailLocation":false,"merchandisingSettings":{"scarcityEnabledOnProductItems":false,"scarcityEnabledOnProductBlocks":false,"scarcityMessageType":"DEFAULT_SCARCITY_MESSAGE","scarcityThreshold":10,"multipleQuantityAllowedForServices":true,"restockNotificationsEnabled":false,"restockNotificationsMailingListSignUpEnabled":false,"relatedProductsEnabled":false,"relatedProductsOrdering":"random","soldOutVariantsDropdownDisabled":false,"productComposerOptedIn":false,"productComposerABTestOptedOut":false},"isLive":false,"multipleQuantityAllowedForServices":true},"useEscapeKeyToLogin":true,"ssBadgeType":1,"ssBadgePosition":4,"ssBadgeVisibility":1,"ssBadgeDevices":1,"pinterestOverlayOptions":{"mode":"disabled"},"ampEnabled":false},"cookieSettings":{"isCookieBannerEnabled":false,"isRestrictiveCookiePolicyEnabled":false,"isRestrictiveCookiePolicyAbsolute":false,"cookieBannerText":"","cookieBannerTheme":"","cookieBannerVariant":"","cookieBannerPosition":"","cookieBannerCtaVariant":"","cookieBannerCtaText":"","cookieBannerAcceptType":"OPT_IN","cookieBannerOptOutCtaText":""},"websiteCloneable":false,"collection":{"title":"Research","id":"564b5638e4b0cfc22513bc9c","fullUrl":"/","type":1,"permissionType":1},"subscribed":false,"appDomain":"squarespace.com","templateTweakable":true,"tweakJSON":{"TGutter":"3%","TMaxWidth":"400","TPerRow":"3","gallery-auto-play":"false","galleryPlaySpeed":"3","hide-thumbnail-titles":"false","outerPadding":"75px","pagePadding":"50px","product-gallery-auto-crop":"false","product-image-auto-crop":"true","thumbnail-layout":"Basic Grid","topPadding":"10px"},"templateId":"564b9dfce4b06621e2bb38d8","templateVersion":"7","pageFeatures":[1,2,4],"gmRenderKey":"QUl6YVN5Q0JUUk9xNkx1dkZfSUUxcjQ2LVQ0QWVUU1YtMGQ3bXk4","templateScriptsRootUrl":"https://static1.squarespace.com/static/ta/564b5107e4b087849452ea4b/0/scripts/","betaFeatureFlags":["nested_categories_migration_enabled","commerce-recaptcha-enterprise","commerce_add_to_cart_rate_limiting","seven_one_header_editor_update","campaigns_user_templates_in_sidebar","ORDER_SERVICE-submit-reoccurring-subscription-order-through-service","commerce_decrease_cart_refreshes","domain_deletion_via_registrar_service","uas_swagger_unauthenticated_session_client","commerce_category_id_discounts_enabled","seven_one_image_effects","domain_locking_via_registrar_service","uas_swagger_session_client","events_panel_70","ORDERS-SERVICE-reset-digital-goods-access-with-service","commerce_product_branching","domains_universal_search","commerce_afterpay_toggle_ineligible","seven-one-main-content-preview-api","member_areas_ga","domains_transfer_flow_hide_preface","commerce_pdp_layout_catalog","commerce_afterpay_pdp","donations_customer_accounts","commerce_product_composer_ab_test_all_users","commerce_restock_notifications","ORDERS-SERVICE-check-digital-good-access-with-service","seven_one_gdpr_opt_out_panel","member_areas_annual_subscriptions","ORDER_SERVICE-submit-subscription-order-through-service","commerce_instagram_product_checkout_links","seven-one-menu-overlay-theme-switcher","seven_one_frontend_render_gallery_section","uas_swagger_site_user_account_client","commerce_activation_experiment_add_payment_processor_card","commerce_pdp_survey_modal","seven_one_frontend_render_page_section","crm_delete_customer_accounts","domains_use_new_domain_connect_strategy","domains_transfer_flow_improvements","commerce_setup_wizard","seven-one-content-preview-section-api","commerce_minimum_order_amount","local_listings","campaigns_new_subscriber_search","generic_iframe_loader_for_campaigns","domain_info_via_registrar_service","seven_one_frontend_render_header_release","commerce_subscription_order_delay","commerce_reduce_cart_calculations","animations_august_2020_new_preset","product_composer_feedback_form_on_save","reduce_general_search_api_traffic","customer_notifications_panel_v2","customer_account_creation_recaptcha","member_areas_discounts","campaigns_single_opt_in","commerce_pdp_layouts","uas_swagger_token_client"],"impersonatedSession":false,"tzData":{"zones":[[-300,"US","E%sT",null]],"rules":{"US":[[1967,2006,null,"Oct","lastSun","2:00","0","S"],[1987,2006,null,"Apr","Sun>=1","2:00","1:00","D"],[2007,"max",null,"Mar","Sun>=8","2:00","1:00","D"],[2007,"max",null,"Nov","Sun>=1","2:00","0","S"]]}}};</script><script type="text/javascript"> SquarespaceFonts.loadViaContext(); Squarespace.load(window);</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="http://www.figlab.com/research?format=rss">
<script type="application/ld+json">{"url":"http://www.figlab.com","name":"Future Interfaces Group","description":"","image":"//static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1610557715131/","@context":"http://schema.org","@type":"WebSite"}</script><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/36/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1605716367774/site.css"><script>Static.COOKIE_BANNER_CAPABLE = true;</script>
<!-- End of Squarespace Headers -->
</head>

<body class="blog-sidebar-display-auto index-thumb-title-position-below index-item-layout-auto thumbnail-layout-basic-grid thumbnails-on-open-page-show-all   layout-style-left page-borders-none underline-body-links gallery-page-controls-tiny-thumbnails  social-icon-style-normal  show-category-navigation product-list-titles-under product-list-alignment-center product-item-size-11-square product-image-auto-crop product-gallery-size-11-square  show-product-price show-product-item-nav product-social-sharing  event-show-past-events event-thumbnails event-thumbnail-size-32-standard event-date-label  event-list-show-cats event-list-date event-list-time event-list-address   event-icalgcal-links  event-excerpts  event-item-back-link     hide-opentable-icons opentable-style-dark newsletter-style-dark small-button-style-solid small-button-shape-square medium-button-style-solid medium-button-shape-square large-button-style-solid large-button-shape-square button-style-solid button-corner-style-square tweak-product-quick-view-button-style-floating tweak-product-quick-view-button-position-bottom tweak-product-quick-view-lightbox-excerpt-display-truncate tweak-product-quick-view-lightbox-show-arrows tweak-product-quick-view-lightbox-show-close-button tweak-product-quick-view-lightbox-controls-weight-light native-currency-code-usd collection-564b5638e4b0cfc22513bc9c collection-type-index collection-layout-default homepage view-list mobile-style-available logo-image" id="collection-564b5638e4b0cfc22513bc9c" data-new-gr-c-s-check-loaded="14.1005.0" data-gr-ext-installed="">

  <div id="canvas">


  <div id="mobileNav" class="">
    <div id="mobileNavWrapper" class="wrapper">
      <nav class="main-nav mobileNav">
        <ul>



              <li class="index-collection active-link">

                  <a href="/">Research</a>

              </li>





              <li class="page-collection">

                  <a href="/about-us">About Us</a>


              </li>





              <li class="page-collection">

                  <a href="/sponsors">Sponsors</a>


              </li>





              <li class="page-collection">

                  <a href="/facilities">Contact</a>


              </li>



        </ul>
      </nav>
    </div>
  </div>
  <div id="mobileMenuLink"><a>Menu</a></div>





    <header id="header">
      <div id="logo" data-content-field="site-title">

        <h1 class="logo landscape" data-shrink-original-size="30" style="letter-spacing: 0.0333333em;" id="yui_3_17_2_1_1618546122048_1478"><a href="/"><img src="//static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1610557715131/?format=1000w" alt="Future Interfaces Group"></a></h1>


      </div>
      <script>
        Y.use('squarespace-ui-base', function(Y) {
          Y.all("#header .logo, #header .logo-subtitle").each(function (text) {
            text.plug(Y.Squarespace.TextShrink, {
              parentEl: Y.one('#header'),
              triggerWidth: 750
            });
          });
        });

        // Show spinner on page load if loading bookmarked url
        if (window.location.hash && window.location.hash !== '#') {
          document.querySelector('body').className += ' index-loading';
        }
      </script>
      <div id="topNav" data-content-field="navigation">
<nav class="main-nav"><ul>


      <li class="index-collection active-link">



            <a href="/">Research</a>



      </li>



      <li class="page-collection">




            <a href="/about-us">About Us</a>







      </li>



      <li class="page-collection">




            <a href="/sponsors">Sponsors</a>







      </li>



      <li class="page-collection">




            <a href="/facilities">Contact</a>







      </li>


</ul>
</nav>
</div>

    </header>

    <div class="page-divider"></div>

    <div class="extra-wrapper page-header">
      <div class="sqs-layout sqs-grid-12 columns-12" data-layout-label="Header Content: Research" data-type="block-field" data-updated-on="1590720745171" id="page-header-564b5638e4b0cfc22513bc9c"><div class="row sqs-row"><div class="col sqs-col-12 span-12"><div class="sqs-block html-block sqs-block-html" data-block-type="2" id="block-db0dcc93556dbb1eb264"><div class="sqs-block-content"><h2 style="white-space:pre-wrap;">The Future Interfaces Group is an interdisciplinary research lab within the <a href="http://hcii.cmu.edu" target="_blank">Human-Computer Interaction Institute</a> at <a href="http://cmu.edu" target="_blank">Carnegie Mellon University</a>.&nbsp;We create new sensing and interface technologies that foster powerful and delightful interactions between humans and computers. These efforts often lie in emerging use modalities, such as smart environments, wearable computing, augmented reality, and gestural interfaces.</h2></div></div></div></div></div>
    </div>

    <section id="page" role="main" data-content-field="main-content">
      <!-- CATEGORY NAV -->

      <div id="projectPages" data-collection-id="">


      <div class="project gallery-project" data-url="/bodyslam/" id="yui_3_17_2_1_1618546122048_137">

        <div class="project-meta">
          <h2 class="project-title">BodySLAM (2020)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Handheld controllers, offering several buttons and six degree-of-freedom tracking, are the most common input approach seen in today’s augmented and virtual reality (AR/VR) systems (e.g., HTC Vive, Oculus Rift). Of course, there are many other facets that could be valuable to digitize, including user body pose, facial expression, skin tone and apparel. Unfortunately, very few AR/VR systems capture these dimensions, and when they do, it is most often via special worn sensors (e.g., instrumented gloves, additional cameras mounted on the headset). Alternatively, external infrastructure can be deployed (e.g., multiple room-mounted cameras) that capture body pose without having to instrument the user. </p><p class="" style="white-space:pre-wrap;">In this work, we take advantage of an emerging use case: co-located, multi-user AR/VR experiences. In such contexts, participants are often able to see each other’s bodies, hands, mouths, apparel, and other visual facets, even though they generally do not see themselves. Using the existing outwards-facing cameras on smartphones and AR/VR headsets (e.g., Microsoft Hololens, Google Cardboard), these visual dimensions can be opportunistically captured and digitized, and then relayed back to their respective users in real time. This is the key insight that motivated our work on BodySLAM. </p><p class="" style="white-space:pre-wrap;">Our system name was inspired by SLAM (simultaneous localization and mapping) approaches to mapping unknown environments. In these systems, many viewpoints are used to reconstruct the geometry of the environment and objects. In a similar vein, BodySLAM uses disparate camera views from many participants to reconstruct the geometric arrangement of bodies in the environment, as well digitize the individual bodies themselves (bodies, hands, mouths, skin and apparel). When a person is seen by two or more user, we can estimate 3D data.</p><p class="" style="white-space:pre-wrap;">We evaluated our system in a multi-part user study, incorporating two tasks and two group sizes. To explore how BodySLAM might scale to larger numbers of people, we ran software simulations in virtual rooms. Although we did not build any demo applications, we note that digitization of bodies has been well motivated in prior work, including uses in social VR and telepresence, entertainment and gaming, and 3D manipulation.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_136">


          <div class="image" id="yui_3_17_2_1_1618546122048_135">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allow=&quot;autoplay; fullscreen&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot; src=&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FreSqz3H7yP8%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DreSqz3H7yP8&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FreSqz3H7yP8%2Fhqdefault.jpg&amp;amp;key=61d05c9d54e8455ea7a9677c366be814&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&amp;amp;wmode=opaque&quot; width=&quot;854&quot; frameborder=&quot;0&quot; title=&quot;YouTube embed&quot; class=&quot;embedly-embed&quot; height=&quot;480&quot;></iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_143"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605715765776-REYYSXAFYBAR4IRDDN2Q/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/image-asset.png" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>BodySLAM: Opportunistic User Digitization in Multi-User AR/VR Experiences</strong></span>
              <span class="image-desc"><p class="" style="white-space:pre-wrap;">Ahuja, K., Goel, M. and Harrison, C. 2020. BodySLAM: Opportunistic Body, Hand and Mouth Tracking in Multi-User AR/VR Experiences. In Proceedings of the 8th ACM Symposium on Spatial User Interaction. (October 30 – November 1, 2020). SUI '20. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/dov/" id="yui_3_17_2_1_1618546122048_166">

        <div class="project-meta">
          <h2 class="project-title">Direction of Voice (2020)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Where a person is looking is an important social cue in human-human interaction, allowing someone to address a particular person in conversation or denote an area of interest. For several decades, human-computer interaction researchers have looked at using gaze data to ease and enhance interactions with computing systems, ranging from social robots to smart environments. However, to capture gaze direction, special sensors must either be worn on the head (unlikely for consumer adoption) or external cameras are used (which can be privacy invasive). </p><p class="" style="white-space:pre-wrap;">In this research, we explored the use of speech as a directional communication modality. In addition to receiving and processing spoken content, we propose that devices also infer the Direction of Voice (DoV). Note this is different from Direction of Arrival (DoA) algorithms, which calculate from where a voice originated. In contrast, DoV calculates the direction along which a voice was projected. </p><p class="" style="white-space:pre-wrap;">Such DoV estimation innately enables voice commands with addressability, in a similar way to gaze, but without the need for cameras. This allows users to easily and naturally interact with diverse ecosystems of voice-enabled devices, whereas today’s voice interactions suffer from multi-device confusion. With DoV estimation providing a disambiguation mechanism, a user can speak to a particular device and have it respond; e.g., a user could ask their smartphone for the time, laptop to play music, smartspeaker for the weather, and TV to play a show. Another benefit of DoV estimation is the potential to dispense with wakewords (e.g., “Hey Siri”, “OK Google”) if devices are confident that they are the intended target for a command. This would also enable general commands – e.g., “up” – to be innately device-context specific (e.g., window blinds, thermostat, television).</p><p class="" style="white-space:pre-wrap;">Our approach relies on fundamental acoustic properties of both human speech and multipath effects in human environments. Our machine learning model leverages features derived from these phenomena to predict both angular direction of voice, and more coarsely, if a user is facing or not facing a device. Our software is lightweight, able to run on a wide variety of consumer devices without having to send audio to the cloud for processing, helping to preserve privacy.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_165">


          <div class="image" id="yui_3_17_2_1_1618546122048_164">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allow=&quot;autoplay; fullscreen&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot; src=&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FTLl-N61LBMg%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DTLl-N61LBMg&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FTLl-N61LBMg%2Fhqdefault.jpg&amp;amp;key=61d05c9d54e8455ea7a9677c366be814&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&amp;amp;wmode=opaque&quot; width=&quot;854&quot; frameborder=&quot;0&quot; title=&quot;YouTube embed&quot; class=&quot;embedly-embed&quot; height=&quot;480&quot;></iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_172"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605716252964-B6G0R11K5W9XUXFK2MTA/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg" data-image-dimensions="1920x1080" data-image-focal-point="0.24577910198107783,0.6429169379592596"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems</strong></span>
              <span class="image-desc"><p class="" style="white-space:pre-wrap;">Ahuja, K., Kong, A., Goel, M. and Harrison, C. 2020. Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (October 20 - 23, 2020). UIST '20. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/vibrocomm/" id="yui_3_17_2_1_1618546122048_195">

        <div class="project-meta">
          <h2 class="project-title">VibroComm (2020)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Inertial Measurement Units (IMUs) with gyroscopic sensors are standard in today’s mobile devices. We show that these sensors can be co-opted for vibroacoustic data reception. Our approach, called VibroComm, requires direct physical contact to a transmitting (i.e., vibrating) surface. This makes interactions targeted and explicit in nature, making it well suited for contexts with many targets or requiring and intent. It also offers an orthogonal dimension of physical security to wireless technologies like Bluetooth and NFC. We achieve a transfer rate over 2000 bits/sec with less than 5% packet loss – an order of magnitude faster than prior IMU-based approaches at a quarter of the loss rate. Published at MobileHCI 2020.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_194">


          <div class="image" id="yui_3_17_2_1_1618546122048_193">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allow=&quot;autoplay; fullscreen&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot; src=&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FgCA-0cPS1eM%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DgCA-0cPS1eM&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FgCA-0cPS1eM%2Fhqdefault.jpg&amp;amp;key=61d05c9d54e8455ea7a9677c366be814&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&amp;amp;wmode=opaque&quot; width=&quot;854&quot; frameborder=&quot;0&quot; title=&quot;YouTube embed&quot; class=&quot;embedly-embed&quot; height=&quot;480&quot;></iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_201"></div>
            <div class="image-meta">
              <span class="image-title"><strong>VibroComm: Using Commodity Gyroscopes for Vibroacoustic Data Reception</strong></span>
              <span class="image-desc"><p class="" style="white-space:pre-wrap;">Xiao, R., Mayer, S. and Harrison, C. 2020. VibroComm: Using Commodity Gyroscopes for Vibroacoustic Data Reception. In Proceedings of the 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services (October 5 - 8, 2020). MobileHCI ’20. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/listen-learner/" id="yui_3_17_2_1_1618546122048_223">

        <div class="project-meta">
          <h2 class="project-title">Listen Learner (2020)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Acoustic activity recognition has emerged as a foundational element for imbuing devices with context-driven capabilities, enabling richer, more assistive, and more accommodating computational experiences. Traditional approaches rely either on custom models trained in situ, or general models pre-trained on preexisting data, with each approach having accuracy and user burden implications. We present Listen Learner, a technique for activity recognition that gradually learns events specific to a deployed environment while minimizing user burden. More specifically, we built an end-to-end system for self-supervised learning of events labelled through one-shot voice interactions. Published at CHI 2020.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_222">


          <div class="image" id="yui_3_17_2_1_1618546122048_221">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allow=&quot;autoplay; fullscreen&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot; src=&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FStF_Is20WcQ%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DStF_Is20WcQ&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FStF_Is20WcQ%2Fhqdefault.jpg&amp;amp;key=61d05c9d54e8455ea7a9677c366be814&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&amp;amp;wmode=opaque&quot; width=&quot;854&quot; frameborder=&quot;0&quot; title=&quot;YouTube embed&quot; class=&quot;embedly-embed&quot; height=&quot;480&quot;></iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_229"></div>
            <div class="image-meta">
              <span class="image-title"><strong>Listen Learner: Automatic Class Discovery &amp; One-Shot Interactions for Acoustic Activity Recognition</strong></span>
              <span class="image-desc"><p class="" style="white-space:pre-wrap;">Wu, J., Harrison, C., Bigham, J. and Laput, G. 2020. Automated Class Discovery and One-Shot Interactions for Acoustic Activity Recognition. In Proceedings of the 38th Annual SIGCHI Conference on Human Factors in Computing Systems. CHI '20. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/wireality-2020/" id="yui_3_17_2_1_1618546122048_251">

        <div class="project-meta">
          <h2 class="project-title">Wireality (2020)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Wireality is a worn VR haptic system that allows for individual joints on the hands to be accurately arrested in 3D space through the use of retractable wires that can be locked. This allows for convincing tangible interactions with large and complex geometries, such as walls, furniture and railings. Our approach is lightweight (11g worn on the hands), low-cost (~$35) and low-power (0.024mWh per actuation). </p><p class="" style="white-space:pre-wrap;"><a href="https://cathy-fang.com/image/wireality/Wireality_CHI_Final_Permission.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_250">


          <div class="image" id="yui_3_17_2_1_1618546122048_249">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allow=&quot;autoplay; fullscreen&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot; src=&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FLzLht9m51XQ%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DLzLht9m51XQ&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FLzLht9m51XQ%2Fhqdefault.jpg&amp;amp;key=61d05c9d54e8455ea7a9677c366be814&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&amp;amp;wmode=opaque&quot; width=&quot;854&quot; frameborder=&quot;0&quot; title=&quot;YouTube embed&quot; class=&quot;embedly-embed&quot; height=&quot;480&quot;></iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_257"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1587751064225-K3AYOSRGW3UWEJ5LZ9LL/ke17ZwdGBToddI8pDm48kECGXzeLompxCB9XLwW2LBh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UW7TDhHdMmhRWA-_rDPuz1BDSURgyO_REA5DNXnKX-BkP7cJNZlDXbgJNE9ef52e8w/image-asset.png" data-image-dimensions="1563x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics</strong></span>
              <span class="image-desc"><p class="" style="white-space:pre-wrap;">Cathy Fang, Yang Zhang, Matthew Dworman, and Chris Harrison. 2020. Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics. In <em>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em> (CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–10. DOI:https://doi.org/10.1145/3313831.3376470</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/worldgaze-2020/" id="yui_3_17_2_1_1618546122048_280">

        <div class="project-meta">
          <h2 class="project-title">WorldGaze (2020)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Contemporary voice assistants require that objects of interest be specified in spoken commands. Of course, users are often looking directly at the object or place of interest – fine-grained, contextual information that is currently unused. We present WorldGaze, a software-only method for smartphones that provides the real-world gaze location of a user that voice agents can utilize for rapid, natural, and precise interactions. We achieve this by simultaneously opening the front and rear cameras of a smartphone. The front-facing camera is used to track the head in 3D, including estimating its direction vector. As the geometry of the front and back cameras are fixed and known, we can raycast the head vector into the 3D world scene as captured by the rear-facing camera. This allows the user to intuitively define an object or region of interest using their head gaze. We started our investigations with a qualitative exploration of competing methods, before developing a functional, real-time implementation. We conclude with an evaluation that shows WorldGaze can be quick and accurate, opening new multimodal gaze+voice interactions for mobile voice agents.</p><p class="" style="white-space:pre-wrap;"><a href="http://sven-mayer.com/paper/worldgaze" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_279">


          <div class="image" id="yui_3_17_2_1_1618546122048_278">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allow=&quot;autoplay; fullscreen&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;true&quot; src=&quot;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FkjACtQK3D-k%3Ffeature%3Doembed&amp;amp;display_name=YouTube&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DkjACtQK3D-k&amp;amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkjACtQK3D-k%2Fhqdefault.jpg&amp;amp;key=61d05c9d54e8455ea7a9677c366be814&amp;amp;type=text%2Fhtml&amp;amp;schema=youtube&amp;amp;wmode=opaque&quot; width=&quot;854&quot; frameborder=&quot;0&quot; title=&quot;YouTube embed&quot; class=&quot;embedly-embed&quot; height=&quot;480&quot;></iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_286"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1586890499726-VEOKDTQQXXRLSN8GB3B9/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Enhancing Mobile Voice Assistants with WorldGaze</strong></span>
              <span class="image-desc"><p class="" style="white-space:pre-wrap;">Mayer, Sven; Laput, Gierad; Harrison, Chris. 2020. Enhancing Mobile Voice Assistants with WorldGaze. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI '20), ACM, New York, NY, USA. DOI: https://dx.doi.org/10.1145/3313831.3376479</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/lightanchors-2019/" id="yui_3_17_2_1_1618546122048_309">

        <div class="project-meta">
          <h2 class="project-title">LightAnchors (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Augmented Reality allows for the overlay of digital information and interactive content onto scenes and objects. In order to provide tight registration of data onto objects in a scene, it is most common for markers to be employed, such as QR Codes and ArUco markers. LightAnchors enables spatially-anchored data in augmented reality applications without special hardware. Unlike most prior tracking methods, which instrument objects with markers, LightAnchors takes advantage of point lights already found in many objects and environments. For example, most electrical appliances now feature small (LED) status lights, and light bulbs are common in indoor and outdoor settings. In addition to leveraging point lights for in-view anchoring, we also co-opt these lights for data transmission, blinking them rapidly to encode binary data. This allows “dumb” devices to become smarter through AR with minimal extra cost. For example, a glue gun can transmit its live temperature and a security camera can broadcast its privacy policy.</p><p class="" style="white-space:pre-wrap;"><a href="https://karan-ahuja.com/assets/docs/paper/lightanchors.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_308">


          <div class="image" id="yui_3_17_2_1_1618546122048_307">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/sdgnX49ZXig?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_315"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903661786-1837K7W06KT48R6S9M0S/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/lightanchors.png" data-image-dimensions="720x720" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>LightAnchors: Appropriating Point Lights for Spatially-Anchored Augmented Reality Interfaces</strong></span>
              <span class="image-desc"><p class="">Karan Ahuja, Sujeath Pareddy, Robert Xiao, Mayank Goel, and Chris Harrison. 2019. LightAnchors: Appropriating Point Lights for Spatially-Anchored Augmented Reality Interfaces. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19). ACM, New York, NY, USA, 189-196. DOI: <a href="https://doi.org/10.1145/3332165.3347884">https://doi.org/10.1145/3332165.3347884</a></p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/sozu-2019/" id="yui_3_17_2_1_1618546122048_338">

        <div class="project-meta">
          <h2 class="project-title">Sozu (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Robust, wide-area sensing of human environments has been a long-standing research goal.&nbsp;In this project, we developed Sozu, a low-cost sensing system that can detect a wide range of events wirelessly, through walls and without line of sight, at whole-building scale. Instead of using batteries, Sozu tags convert energy from activities that they sense into RF broadcasts, acting like miniature self-powered radio stations. We describe the results from a series of iterative studies, culminating in a deployment study with 30 instrumented objects. Results show that Sozu is very accurate, with true positive event detection exceeding 99%, with almost no false positives. Beyond event detection, we show that Sozu can be extended to detect richer signals, such as the state, intensity, count, and rate of events. Source code of this project can be found here: https://github.com/FIGLAB/Sozu</p><p class="" style="white-space:pre-wrap;"><a href="https://yangzhang.dev/research/Sozu/Sozu.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_337">


          <div class="image" id="yui_3_17_2_1_1618546122048_336">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/wbq-eOOIPyw?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_344"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572881919944-4HVJCY6P8CW68AM3QPWZ/ke17ZwdGBToddI8pDm48kBIulzeX5j9NT9xsNh53_SlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIPNYiFL7BnXyjRMbW9h8dM04P3OJO1lsaFgDfvoR4Ro8KMshLAGzx4R3EDFOm1kBS/image-asset.jpeg" data-image-dimensions="811x811" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Sozu: Self-Powered Radio Tags for Building-Scale Activity Sensing</strong></span>
              <span class="image-desc"><p class="">Yang Zhang, Yasha Iravantchi, Haojian Jin, Swarun Kumar, and Chris Harrison. 2019. Sozu: Self-Powered Radio Tags for Building-Scale Activity Sensing. In <em>Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology</em> (UIST '19). ACM, New York, NY, USA, 973-985. DOI: https://doi.org/10.1145/3332165.3347952</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/actitouch-2019/" id="yui_3_17_2_1_1618546122048_367">

        <div class="project-meta">
          <h2 class="project-title">ActiTouch (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Contemporary AR/VR systems use in-air gestures or handheld controllers for interactivity. This overlooks the skin as a convenient surface for tactile, touch-driven interactions, which are generally more accurate and comfortable than free space interactions. In response, we developed ActiTouch, a wearable system that allows users to use their hands and arms as readily available touch input surfaces for AR and VR, opening a new interaction opportunity beyond conventional controllers and in-air gestures. We invented a powerful sensor fusion method which combines an electrical method with computer vision. This enables precise on-skin touch segmentations, which uniquely enables many fine-grained touch interactions such as scrolling and swiping.</p><p class="" style="white-space:pre-wrap;"><a href="https://yangzhang.dev/research/ActiTouch/ActiTouch.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_366">


          <div class="image" id="yui_3_17_2_1_1618546122048_365">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/ykjMiekzxyA?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_373"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572883502552-TS0H8518ZXGHNSM5LB05/ke17ZwdGBToddI8pDm48kJxzbK9g66s9755W1mft6Jp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmp0iglcLYVjOea_4STb6GncJu24-mjs-PD1sbKDlJam1ANT7JgEFYgkN6X0N795Ud/thumbnail.png" data-image-dimensions="1095x1096" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>ActiTouch: Robust Touch Detection for On-Skin AR/VR Interactions</strong></span>
              <span class="image-desc"><p class="">Yang Zhang, Wolf Kienzle, Yanjun Ma, Shiu S. Ng, Hrvoje Benko, and Chris Harrison. 2019. ActiTouch: Robust Touch Detection for On-Skin AR/VR Interfaces. In <em>Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology</em> (UIST '19). ACM, New York, NY, USA, 1151-1159. DOI: https://doi.org/10.1145/3332165.3347869</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/mecap-2019/" id="yui_3_17_2_1_1618546122048_396">

        <div class="project-meta">
          <h2 class="project-title">MeCap (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Low-cost, smartphone-powered VR/AR headsets are becoming more popular. These basic devices – little more than plastic or cardboard shells – lack advanced features, such as controllers for the hands, limiting their interactive capability. Moreover, even high-end consumer headsets lack the ability to track the body and face. For this reason, interactive experiences like social VR are underdeveloped. We introduce MeCap, which enables commodity VR headsets to be augmented with powerful motion capture (“MoCap”) and user-sensing capabilities at very low cost (under $5). Using only a pair of hemi-spherical mirrors and the existing rear-facing camera of a smartphone, MeCap provides real-time estimates of a wearer’s 3D body pose, hand pose, facial expression, physical appearance and surrounding environment – capabilities which are either absent in contemporary VR/AR systems or which require specialized hardware and controllers. We evaluate the accuracy of each of our tracking features, the results of which show imminent feasibility.</p><p class="" style="white-space:pre-wrap;"><a href="https://karan-ahuja.com/assets/docs/paper/mecap.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_395">


          <div class="image" id="yui_3_17_2_1_1618546122048_394">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/mR84i0HvzwM?start=1&amp;amp;wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_402"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903441987-YZOCGTNYJVUDA6XNEUNK/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/mecap.png" data-image-dimensions="720x720" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>MeCap: Whole-Body Digitization for Low-Cost VR/AR Headsets</strong></span>
              <span class="image-desc"><p class="">Karan Ahuja, Chris Harrison, Mayank Goel, and Robert Xiao. 2019. MeCap: Whole-Body Digitization for Low-Cost VR/AR Headsets. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19). ACM, New York, NY, USA, 453-462. DOI: <a href="https://doi.org/10.1145/3332165.3347889">https://doi.org/10.1145/3332165.3347889</a></p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/surfacesight/" id="yui_3_17_2_1_1618546122048_425">

        <div class="project-meta">
          <h2 class="project-title">SurfaceSight (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">IoT appliances are gaining consumer traction, from smart thermostats to smart speakers. These devices generally have limited user interfaces, most often small butons and touchscreens, or rely on voice control. Further, these devices know little about their surroundings – unaware of objects, people and activities happening around them. Consequently, interactions with these “smart” devices can be cumbersome and limited. We describe SurfaceSight, an approach that enriches IoT experiences with rich touch and object sensing, offering a complementary input channel and increased contextual awareness. For sensing, we incorporate LIDAR into the base of IoT devices, providing an expansive, ad hoc plane of sensing just above the surface on which devices rest. We can recognize and track a wide array of objects, including finger input and hand gestures. We can also track people and estimate which way they are facing. We evaluate the accuracy of these new capabilities and illustrate how they can be used to power novel and contextually-aware interactive experiences.</p><p class="" style="white-space:pre-wrap;"><a href="https://www.gierad.com/assets/surfacesight/surfacesight.pdf" target="">Download paper.</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_424">


          <div class="image" id="yui_3_17_2_1_1618546122048_423">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/pLWdvpg8BMg?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_431"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557219886228-2MC14FL20UOS5ZZOA07T/ke17ZwdGBToddI8pDm48kBL7y0DpH_e0bbX7enFn0u57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UT4ke-2faLbDJ2EQYXZquaaYAmln24LeHvojPJ-RAjfURBpFeAJxHLqEa5Otf4mhGw/thumb.jpg" data-image-dimensions="1920x1920" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences</strong></span>
              <span class="image-desc"><p class="">Gierad Laput and Chris Harrison. 2019. SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19). ACM, New York, NY, USA, Paper 329, 12 pages. DOI: https://doi.org/10.1145/3290605.3300559</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/edusense-2019/">

        <div class="project-meta">
          <h2 class="project-title">EduSense (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Providing university teachers with high-quality opportunities for professional development cannot happen without data about the classroom environment. Currently, the most effective mechanism is for an expert to observe one or more lectures and provide personalized formative feedback to the instructor. Of course, this is expensive and unscalable, and perhaps most critically, precludes a continuous learning feedback loop for the instructor. In this paper, we present the culmination of two years of research and development on EduSense, a comprehensive sensing system that produces a plethora of theoretically-motivated visual and audio features correlated with effective instruction, which could feed professional development tools in much the same way as a Fitbit sensor reports step count to an end user app. Although previous systems have demonstrated some of our features in isolation, EduSense is the first to unify them into a cohesive, real-time, in-the-wild evaluated, and practically-deployable system. Our two studies quantify where contemporary machine learning techniques are robust, and where they fall short, illuminating where future work remains to bring the vision of automated classroom analytics to reality.</p><p class="" style="white-space:pre-wrap;"><a href="https://karan-ahuja.com/assets/docs/paper/edusense.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list">


          <div class="image">

              <img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903942626-FO531JB691JZ71HSW8HG/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903942626-FO531JB691JZ71HSW8HG/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png" data-image-dimensions="720x720" data-image-focal-point="0.5,0.5" alt="EduSense: Practical Classroom Sensing at Scale" class="loading" data-load="false">
              <noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903942626-FO531JB691JZ71HSW8HG/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png?format=500w"></noscript>


            <div class="image-meta">
              <span class="image-title"><strong>EduSense: Practical Classroom Sensing at Scale</strong></span>
              <span class="image-desc"><p class="">Karan Ahuja, Dohyun Kim, Franceska Xhakaj, Virag Varga, Anne Xie, Stanley Zhang, Jay Eric Townsend, Chris Harrison, Amy Ogan, and Yuvraj Agarwal. 2019. EduSense: Practical Classroom Sensing at Scale. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 3, Article 71 (September 2019), 26 pages. DOI: <a href="https://doi.org/10.1145/3351229">https://doi.org/10.1145/3351229</a></p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/handactivities/" id="yui_3_17_2_1_1618546122048_454">

        <div class="project-meta">
          <h2 class="project-title">Hand Activities (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Capturing fine-grained hand activity could make computational experiences more powerful and contextually aware. Indeed, philosopher Immanuel Kant argued, "the hand is the visible part of the brain." However, most prior work has focused on detecting whole-body activities, such as walking, running and bicycling. In this work, we explore the feasibility of sensing hand activities from commodity smart- watches, which are the most practical vehicle for achieving this vision. Our investigations started with a 50 participant, in-the-wild study, which captured hand activity labels over nearly 1000 worn hours. We then studied this data to scope our research goals and inform our technical approach. We conclude with a second, in-lab study that evaluates our classification stack, demonstrating 95.2% accuracy across 25 hand activities. Our work highlights an underutilized, yet highly complementary contextual channel that could un- lock a wide range of promising applications.</p><p class="" style="white-space:pre-wrap;"><a href="https://www.gierad.com/projects/handactivities/" target="">Download paper.</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_453">


          <div class="image" id="yui_3_17_2_1_1618546122048_452">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/a-ImMjOrbbI?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_460"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557218838661-KUUDGYT8XLPXAJ70MZWZ/ke17ZwdGBToddI8pDm48kKY-G6HndK7v2FlTyTIJQI57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UWBFTp-jyz-u-yJnrxahDVdFAjR2sOZTixm23ygffcf2JQf5RnFi622klzb8JVxQkQ/thumb.jpg" data-image-dimensions="1814x1814" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Sensing Fine-Grained Hand Activity with Smartwatches</strong></span>
              <span class="image-desc"><p class="">Gierad Laput and Chris Harrison. 2019. Sensing Fine-Grained Hand Activity with Smartwatches. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19). ACM, New York, NY, USA, Paper 338, 13 pages. DOI: https://doi.org/10.1145/3290605.3300568</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/beamband/" id="yui_3_17_2_1_1618546122048_483">

        <div class="project-meta">
          <h2 class="project-title">BeamBand (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">BeamBand is a wrist-worn system that uses ultrasonic beamforming for hand gesture sensing. Using an array of small transducers, arranged on the wrist, we can ensem-ble acoustic wavefronts to project acoustic energy at spec-ified angles and focal lengths. This allows us to interro-gate the surface geometry of the hand with inaudible sound in a raster-scan-like manner, from multiple view-points. We use the resulting, characteristic reflections to recognize hand pose at 8 FPS. In our user study, we found that BeamBand supports a six-class hand gesture set at 94.6% accuracy. Even across sessions, when the sensor is removed and reworn later, accuracy remains high: 89.4%. We describe our software and hardware, and future ave-nues for integration into devices such as smartwatches and VR controllers.</p><p class="" style="white-space:pre-wrap;"><a href="/s/beamband.pdf">Download Paper.</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_482">


          <div class="image" id="yui_3_17_2_1_1618546122048_481">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/jhY4NsIW2kQ?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_489"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020686362-TC2O63PYYS1E1HV0ZMW8/ke17ZwdGBToddI8pDm48kBVDUY_ojHUJPbTAKvjNhBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmuRu0Pfuf5F0yH2Y0n9Wb2uSXfk79yzklavJJ48t78dp27rAT2psOBuL_FmvkUhyS/beamband-thumb.png" data-image-dimensions="1440x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>BeamBand: Hand Gesture Sensing with Ultrasonic Beamforming</strong></span>
              <span class="image-desc"><p class="">Yasha Iravantchi, Mayank Goel, and Chris Harrison. 2019. BeamBand: Hand Gesture Sensing with Ultrasonic Beamforming. In&nbsp;<em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>&nbsp;(CHI '19). ACM, New York, NY, USA, Paper 15, 10 pages. DOI: https://doi.org/10.1145/3290605.3300245</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/interferi/" id="yui_3_17_2_1_1618546122048_512">

        <div class="project-meta">
          <h2 class="project-title">Interferi (2019)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Interferi is an on-body gesture sensing technique using acoustic interferometry. We use ultrasonic transducers resting on the skin to create acoustic interference patterns inside the wearer’s body, which interact with anatomical features in complex, yet characteristic ways. We focus on two areas of the body with great expressive power: the hands and face. For each, we built and tested a series of worn sensor configurations, which we used to identify useful transducer arrangements and machine learning fea-tures. We created final prototypes for the hand and face, which our study results show can support eleven- and nine-class gestures sets at 93.4% and 89.0% accuracy, re-spectively. We also evaluated our system in four continu-ous tracking tasks, including smile intensity and weight estimation, which never exceed 9.5% error. We believe these results show great promise and illuminate an inter-esting sensing technique for HCI applications.</p><p class="" style="white-space:pre-wrap;"><a href="/s/interferi.pdf">Download Paper.</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_511">


          <div class="image" id="yui_3_17_2_1_1618546122048_510">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/_nMauMXDqf8?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_518"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020497815-WC0ADXGYWFIQ3OV78W9B/ke17ZwdGBToddI8pDm48kAegX-1irUL6qWVp5YHdPlZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0mjKWc80r5PD-660Rc3KKl-EIGwdh7NUl8wqqSWoiGqWWkt1uV2bgdYZttL59vHUoQ/interferi-thumb.png" data-image-dimensions="2500x1404" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Interferi: Gesture Sensing using On-Body Acoustic Interferometry</strong></span>
              <span class="image-desc"><p class="">Yasha Iravantchi, Yang Zhang, Evi Bernitsas, Mayank Goel, and Chris Harrison. 2019. Interferi: Gesture Sensing using On-Body Acoustic Interferometry. In&nbsp;<em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>&nbsp;(CHI '19). ACM, New York, NY, USA, Paper 276, 13 pages. DOI: https://doi.org/10.1145/3290605.3300506</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/eyespyvr-2018/" id="yui_3_17_2_1_1618546122048_541">

        <div class="project-meta">
          <h2 class="project-title">EyeSpyVR (2018)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Low cost virtual reality headsets powered by smartphones are becoming ubiquitous. Their unique position on the user's face opens interesting opportunities for interactive sensing. We describe EyeSpyVR, a software-only eye sensing approach for smartphone-based VR, which uses a phone's front facing camera as a sensor and its display as a passive illuminator. Our proof-of-concept system, using a commodity smartphone, enables four sensing modalities: detecting when the VR headset is worn, detecting blinks, recognizing the wearer's identity, and coarse gaze tracking - features typically found in high-end or specialty VR headsets. We describe our implementation and results from a 70 participant user study. </p><p class="" style="white-space:pre-wrap;"><a href="https://karan-ahuja.com/assets/docs/paper/eyespyvr.pdf" target="">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_540">


          <div class="image" id="yui_3_17_2_1_1618546122048_539">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/2ec-9yR1KQw?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_547"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1582627177209-3R05EHP1P3QWSXIOZJNS/ke17ZwdGBToddI8pDm48kNXCUrrKjetGVMFIkBcnqREUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKc3XMixWOGMhAQ0lbIR9TAN-1tksE3g3DAinfzJCNIRh6Vo73cC2iPYZg8gDdHNqVP/eyespyvr.png" data-image-dimensions="1202x676" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>EyeSpyVR: Interactive Eye Sensing Using Off-the-Shelf, Smartphone-Based VR Headsets</strong></span>
              <span class="image-desc"><p class="">Karan Ahuja, Rahul Islam, Varun Parashar, Kuntal Dey, Chris Harrison, and Mayank Goel. 2018. EyeSpyVR: Interactive Eye Sensing Using Off-the-Shelf, Smartphone-Based VR Headsets. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 2, Article 57 (July 2018), 10 pages. DOI: <a href="https://doi.org/10.1145/3214260">https://doi.org/10.1145/3214260</a></p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/ubicoustics/" id="yui_3_17_2_1_1618546122048_570">

        <div class="project-meta">
          <h2 class="project-title">Ubicoustics (2018)</h2>
          <div class="project-description"><p style="white-space: pre-wrap;">Despite sound being a rich source of information, computing devices with microphones do not leverage audio to glean useful insights about their physical and social context. For example, a smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a user is doing in a kitchen—a missed opportunity. In this work, we describe a novel, real-time, sound-based activity recognition system. We start by taking an existing, state-of-the-art sound labeling model, which we then tune to classes of interest by drawing data from professional sound effect libraries traditionally used in the entertainment industry. These well-labeled and high-quality sounds are the perfect atomic unit for data augmentation, including amplitude, reverb, and mixing, allowing us to exponentially grow our tuning data in realistic ways. We quantify the performance of our approach across a range of environments and device categories and show that microphone-equipped computing devices already have the requisite capability to unlock real-time activity recognition comparable to human accuracy.</p><p style="white-space: pre-wrap;"><a href="/s/ubicoustics.pdf">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_569">


          <div class="image" id="yui_3_17_2_1_1618546122048_568">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/N5ZaBeB07u4?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_576"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935676369-IVO8NC00OR9S1GVYA0FQ/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/VideoThumbnailLarger.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Ubicoustics: Plug-and-Play Acoustic Activity Recognition</strong></span>
              <span class="image-desc"><p>Gierad Laput, Karan Ahuja, Mayank Goel, and Chris Harrison. 2018. Ubicoustics: Plug-and-Play Acoustic Activity Recognition. In The 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18). ACM, New York, NY, USA, 213-224. DOI: https://doi.org/10.1145/3242587.3242609</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/vibrosight-2018/" id="yui_3_17_2_1_1618546122048_599">

        <div class="project-meta">
          <h2 class="project-title">Vibrosight (2018)</h2>
          <div class="project-description"><p class="" style="white-space:pre-wrap;">Smart and responsive environments rely on the ability to detect physical events, such as appliance use and human activities. Currently, to sense these types of events, one must either upgrade to "smart" appliances, or attach aftermarket sensors to existing objects. These approaches can be expensive, intrusive and inflexible. In this work, we present Vibrosight, a new approach to sense activities across entire rooms using long-range laser vibrometry. Unlike a microphone, our approach can sense physical vibrations at one specific point, making it robust to interference from other activities and noisy environments. This property enables detection of simultaneous activities, which has proven challenging in prior work. Through a series of evaluations, we show that Vibrosight can offer high accuracies at long range, allowing our sensor to be placed in an inconspicuous location. We also explore a range of additional uses, including data transmission, sensing user input and modes of appliance operation, and detecting human movement and activities on work surfaces. Source code of this project can be found here: https://github.com/FIGLAB/vibrosight</p><p class="" style="white-space:pre-wrap;"><a href="/s/vibrosight.pdf">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_598">


          <div class="image" id="yui_3_17_2_1_1618546122048_597">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/51XaZDki6yg?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_605"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935352512-VRVCZG7ZOVNQZRTZW5FU/ke17ZwdGBToddI8pDm48kNKU_v8gJAcxDrmB-soKvj1Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVH7wdpQi_gwH_-rfgB8xc3aCDYU5QsKfHvKofLxtwAwA5XleA9PsoOHujT9UMkA80c/image-asset.jpeg" data-image-dimensions="480x360" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Vibrosight: Long-Range Vibrometry for Smart Environment Sensing</strong></span>
              <span class="image-desc"><p>Yang Zhang, Gierad Laput and Chris Harrison. 2018. Vibrosight: Long-Range Vibrometry for Smart Environment Sensing. In Proceedings of the 31th Annual ACM Symposium on User interface Software and Technology (Berlin, Germany, October 15-18, 2018). UIST '18. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/wall-2018/" id="yui_3_17_2_1_1618546122048_628">

        <div class="project-meta">
          <h2 class="project-title">Wall++ (2018)</h2>
          <div class="project-description"><p>Human environments are typified by walls -- homes, offices, schools, museums, hospitals and pretty much every indoor context one can imagine has walls. In many cases, they make up a majority of readily accessible indoor surface area, and yet they are static -- their primary function is to be a wall, separating spaces and hiding infrastructure. We present Wall++, a low-cost sensing approach that allows walls to become a smart infrastructure. Instead of merely separating spaces, walls can now enhance rooms with sensing and interactivity. Our wall treatment and sensing hardware can track users' touch and gestures, as well as estimate body pose if they are close. By capturing airborne electromagnetic noise, we can also detect what appliances are active and where they are located. Through a series of evaluations, we demonstrate Wall++ can enable robust room-scale interactive and context-aware applications. Published at CHI 2018.</p><p><a href="/s/wall.pdf">Download paper</a></p><p data-rte-preserve-empty="true"></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_627">


          <div class="image" id="yui_3_17_2_1_1618546122048_626">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/175LB2OiMHs?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_634"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524953232010-1SQU4YNCPJUOUFJXU2MU/ke17ZwdGBToddI8pDm48kNKU_v8gJAcxDrmB-soKvj1Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVH7wdpQi_gwH_-rfgB8xc3aCDYU5QsKfHvKofLxtwAwA5XleA9PsoOHujT9UMkA80c/image-asset.jpeg" data-image-dimensions="480x360" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Wall++: Room-Scale Interactive and Context-Aware Sensing</strong></span>
              <span class="image-desc"><p>Yang Zhang, Chouchang(Jack) Yang, Scott E. Hudson, Chris Harrison and Alanson Sample. 2018. Wall++: Room-Scale Interactive and Context-Aware Sensing. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA. DOI: https://doi.org/10.1145/3173574.3173847</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/pulp-nonfiction-2018/" id="yui_3_17_2_1_1618546122048_657">

        <div class="project-meta">
          <h2 class="project-title">Pulp Nonfiction (2018)</h2>
          <div class="project-description"><p>Paper continues to be a versatile and indispensable material in the 21st century. Of course, paper is a passive medium with no inherent interactivity, precluding us from computationally-enhancing a wide variety of paper-based activities. In this work, we present a new technical approach for bringing the digital and paper worlds closer together, by enabling paper to track finger input and also drawn input with writing implements. Importantly, for paper to still be considered paper, our method had to be very low cost. This necessitated research into materials, fabrication methods and sensing techniques. We describe the outcome of our investigations and show that our method can be sufficiently low-cost and accurate to enable new interactive opportunities with this pervasive and venerable material. Published at CHI 2018.</p><p><a href="/s/lyp4gk2u1pjssk3pzcny1yuwp0jslz">Download paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_656">


          <div class="image" id="yui_3_17_2_1_1618546122048_655">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/Y1Q0QCPdZys?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_663"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524952986061-1KEH627ZY5A65DR2W5NG/ke17ZwdGBToddI8pDm48kNKU_v8gJAcxDrmB-soKvj1Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVH7wdpQi_gwH_-rfgB8xc3aCDYU5QsKfHvKofLxtwAwA5XleA9PsoOHujT9UMkA80c/image-asset.jpeg" data-image-dimensions="480x360" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Pulp Nonfiction: Low-Cost Touch Tracking for Paper</strong></span>
              <span class="image-desc"><p>Yang Zhang and Chris Harrison. 2018. Pulp Nonfiction: Low-Cost Touch Tracking for Paper. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA. DOI: https://doi.org/10.1145/3173574.3173691</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/lumiwatch-2018/" id="yui_3_17_2_1_1618546122048_686">

        <div class="project-meta">
          <h2 class="project-title">LumiWatch (2018)</h2>
          <div class="project-description"><p>Compact, worn computers with projected, on-skin touch interfaces have been a long-standing yet elusive goal, largely written off as science fiction. Such devices offer the potential to mitigate the significant human input/output bottleneck inherent in worn devices with small screens. In this work, we present the first, fully-functional and self-contained projection smartwatch implementation, containing the requisite compute, power, projection and touch-sensing capabilities. Our watch offers roughly 40 cm² of interactive surface area – more than five times that of a typical smartwatch display. We demonstrate continuous 2D finger tracking with interactive, rectified graphics, transforming the arm into a touchscreen. We discuss our hardware and software implementation, as well as evaluation results regarding touch accuracy and projection visibility. Published at CHI 2018.</p><p><a href="/s/3djt1yhdq9b3jpuwjrpvo7njf7h4zx">Download paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_685">


          <div class="image" id="yui_3_17_2_1_1618546122048_684">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/VJNMrulWJ3k?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_692"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524952517182-FYFH3MMCTI4M0BRYS3D0/ke17ZwdGBToddI8pDm48kNKU_v8gJAcxDrmB-soKvj1Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVH7wdpQi_gwH_-rfgB8xc3aCDYU5QsKfHvKofLxtwAwA5XleA9PsoOHujT9UMkA80c/image-asset.jpeg" data-image-dimensions="480x360" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>LumiWatch: On-Arm Projected Graphics and Touch Input</strong></span>
              <span class="image-desc"><p>Xiao, R., Cao, T., Guo, N., Zhuo, J., Zhang, Y. and Harrison, C. 2018. LumiWatch: On-Arm Projected Graphics and Touch Input. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). ACM, New York, NY, USA, Paper 95, 11 pages. DOI:&nbsp;10.1145/3173574.3173669</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/desktopography/" id="yui_3_17_2_1_1618546122048_715">

        <div class="project-meta">
          <h2 class="project-title">Desktopography (2017)</h2>
          <div class="project-description"><p>Systems for providing mixed physical-virtual interaction on desktop surfaces have been proposed for decades, though no such systems have achieved widespread use. One major factor contributing to this lack of acceptance may be that these systems are not designed for the variety and complexity of actual work surfaces, which are often in flux and cluttered with physical objects. In this project, we use an elicitation study and interviews to synthesize a list of ten interactive behaviors that desk-bound, digital interfaces should implement to support responsive cohabitation with physical objects. As a proof of concept, we implemented these interactive behaviors in a working augmented desk system, demonstrating their imminent feasibility.</p><p><a href="/s/desktopography.pdf">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_714">


          <div class="image" id="yui_3_17_2_1_1618546122048_713">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/L5mCxfjk6hc?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_721"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1511968018892-08UBFP2YDAC26Q7XE303/ke17ZwdGBToddI8pDm48kLtN5e6yN0zv2sXJfg70D24UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dqdBgw1X-01XsuwO7md6v74ORqr-BhWOGF_r6LtNcKfECjLISwBs8eEdxAxTptZAUg/Screen+Shot+2017-11-29+at+10.02.39+AM.png" data-image-dimensions="1761x805" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Desktopography: Supporting Responsive Cohabitation Between Virtual Interfaces and Physical Objects</strong></span>
              <span class="image-desc"><p>Xiao, R., Hudson, S.E. and Harrison, C. 2017. Supporting Responsive Cohabitation Between Virtual Interfaces and Physical Objects on Everyday Surfaces. In Proceedings of the 9th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (Lisbon, Portugal, June 26 – 29, 2017). EICS ’17. ACM, New York, NY. Article 11.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/synthetic-sensors-2017/" id="yui_3_17_2_1_1618546122048_744">

        <div class="project-meta">
          <h2 class="project-title">Synthetic Sensors (2017)</h2>
          <div class="project-description"><p>The promise of smart environments and the Internet of Things (IoT) relies on robust sensing of diverse environmental facets.&nbsp;In this work, we explore the notion of general-purpose sensing, wherein a single, highly capable sensor can indirectly monitor a large context, without direct instrumentation of objects. Further, through what we call Synthetic Sensors, we can virtualize raw sensor data into actionable feeds, whilst simultaneously mitigating immediate privacy issues. We deployed our system across many months and environments, the results of which show the versatility, accuracy and potential of this approach.</p><p><a href="/s/syntheticsensors.pdf">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_743">


          <div class="image" id="yui_3_17_2_1_1618546122048_742">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/aqbKrrru2co?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_750"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967957339-F6WGHX3987EVOGLC4ZQJ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/00_a_hardware_closeup.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Zhang, Y. and Harrison, C. 2017. Synthetic Sensors: Towards General-Purpose Sensing. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 3986-3999.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/electrick-2017/" id="yui_3_17_2_1_1618546122048_773">

        <div class="project-meta">
          <h2 class="project-title">Electrick (2017)</h2>
          <div class="project-description"><p>Electrick is a low-cost and versatile sensing technique that enables touch input on a wide variety of objects and surfaces, whether small or large, flat or irregular. This is achieved by using electric field tomography in concert with an electrically conductive material, which can be easily and cheaply added to objects and surfaces. We show that our technique is compatible with commonplace manufacturing methods, such as spray/brush coating, vacuum forming, and casting/molding – enabling a wide range of possible uses and outputs. Published at CHI 2017.</p><p><a href="/s/electrick.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_772">


          <div class="image" id="yui_3_17_2_1_1618546122048_771">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/38h4-5FDdV4?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_779"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1495033752366-8EK0WHCGWCL42FDCKX6Z/ke17ZwdGBToddI8pDm48kOxLRGAfd8DHyn-VO-OS79h7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0usXCOu1ni7cqT7DpdWG1QXsxC4JUKCTfgXShWWMRL-jMKpWpWZXzvRmlf_9uuK6qg/electrick2.png" data-image-dimensions="2500x1400" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Zhang, Y., Laput, G. and Harrison, C. 2017. Electrick: Low-Cost Touch Sensing Using Electric Field Tomography. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 1-14.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/deus-em-machina-2017/" id="yui_3_17_2_1_1618546122048_802">

        <div class="project-meta">
          <h2 class="project-title">Deus EM Machina (2017)</h2>
          <div class="project-description"><p>Homes, offices and many other environments will be increasingly saturated with connected, computational appliances, forming the “Internet of Things” (IoT). At present, most of these devices rely on mechanical inputs, webpages, or smartphone apps for control. We propose an approach where users simply tap a smartphone to an appliance to discover and rapidly utilize contextual functionality. To achieve this, our prototype smartphone recognizes physical contact with uninstrumented appliances, and summons appliance-specific interfaces and contextually relevant functionality. Published at CHI 2017.</p><p><a href="/s/emphone.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_801">


          <div class="image" id="yui_3_17_2_1_1618546122048_800">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/eInfzdZ-9fE?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_808"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967499324-T8HCVB9Y0RPLK106QUIV/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/lightApp+copy.JPG" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Xiao, R., Laput, G., Zhang, Y. and Harrison, C. 2017. Deus EM Machina: On-Touch Contextual Functionality for Smart IoT Appliances. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 4000-4008.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/direct-2016/" id="yui_3_17_2_1_1618546122048_831">

        <div class="project-meta">
          <h2 class="project-title">DIRECT (2016)</h2>
          <div class="project-description"><p>Many research systems have demonstrated that depth cameras, combined with projectors for output, can turn nearly any reasonably flat surface into an ad hoc, touch-sensitive display. However, even with the latest generation of depth cameras, it has been difficult to obtain sufficient sensing fidelity across a table-sized surface to get much beyond a proof-of-concept demonstration. In this research, we present DIRECT, a novel touch-tracking algorithm that merges depth and infrared imagery captured by a commodity sensor. Our results show that our technique boosts touch detection accuracy by 15% and reduces positional error by 55% compared to the next best-performing technique in the literature.</p><p><a href="/s/direct.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_830">


          <div class="image" id="yui_3_17_2_1_1618546122048_829">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/w6RtfrczmYQ?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_837"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967379595-YSNWXIRVQQYKZUMMNDGU/ke17ZwdGBToddI8pDm48kG3JlL_fc-X2clqdPGDYShBZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHmcx2YP9nwcfOfkCWtYM4VogHaV5KVN10h6QfYCn6Zu455cz7JN3lP_33j1ut53s8/image-asset.jpeg" data-image-dimensions="477x265" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Xiao, R., Hudson, S. E. and Harrison, C. 2016. DIRECT: Making Touch Tracking on Ordinary Surfaces Practical with Hybrid Depth-Infrared Sensing. In Proceedings of the 11th ACM International Conference on Interactive Surfaces and Spaces (Niagara Falls, Canada, November 6 - 9, 2016). ISS '16. ACM, New York, NY. 85-94.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/capcam-2016/" id="yui_3_17_2_1_1618546122048_860">

        <div class="project-meta">
          <h2 class="project-title">CapCam (2016)</h2>
          <div class="project-description"><p>We present CapCam, a novel technique that enables smartphones (and similar devices) to establish quick, ad-hoc connections with a host touchscreen device, simply by pressing a device to the screen’s surface. Pairing data, used to bootstrap a conventional wireless connection, is transmitted optically to the phone’s rear camera. This approach utilizes the near-ubiquitous rear camera on smart devices, making it applicable to a wide range of devices, both new and old. CapCam also tracks phones’ physical positions on the host capacitive touchscreen without any instrumentation, enabling a wide range of targeted and spatial interactions.&nbsp;</p><p><a href="/s/capcam.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_859">


          <div class="image" id="yui_3_17_2_1_1618546122048_858">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/9ERZaT0X-6M?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;>
</iframe>" data-provider-name="YouTube" id="yui_3_17_2_1_1618546122048_866"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967052179-MN9YRCWLORVBVIFUOC94/ke17ZwdGBToddI8pDm48kPw0bXfz3hjgnTAlZZLoJIl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UegSNDBKjtoREifFChppomv_7sZ1DyunrJoaxYrOLmJqGG0XwADNPuScWbN_Q97KlA/kbd.jpg" data-image-dimensions="2371x1200" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Xiao, R., Hudson, S. E. and Harrison, C. 2016. CapCam: Enabling Rapid, Ad-Hoc, Position-Tracked Interactions Between Devices. In Proceedings of the 11th ACM International Conference on Interactive Surfaces and Spaces (Niagara Falls, Canada, November 6 - 9, 2016). ISS '16. ACM, New York, NY. 169-178.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/viband-2016/" id="yui_3_17_2_1_1618546122048_889">

        <div class="project-meta">
          <h2 class="project-title">ViBand (2016)</h2>
          <div class="project-description"><p>Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Additionally, their position on the wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch’s existing accelerometer, enabling many new applications. For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps. Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling object recognition. Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body.</p><p><a href="/s/aurasense.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_888">


          <div class="image" id="yui_3_17_2_1_1618546122048_887">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/Poi0MeASmuY?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_895"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535799488-SV2QJKPFAWO9VDIN2B7E/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/VideoThumbnail.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Xiao, R. and Harrison, C. 2016. ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. To appear in Proceedings of the 29th Annual ACM Symposium on User interface Software and Technology (Tokyo, Japan, October 16 - 19, 2016). UIST '16. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/aurasense-2016/" id="yui_3_17_2_1_1618546122048_918">

        <div class="project-meta">
          <h2 class="project-title">AuraSense (2016)</h2>
          <div class="project-description"><p>AuraSense enables rich, around-device, smartwatch interactions using electric field sensing. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities. We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above the watchface and touchscreen-like finger tracking on the skin. We quantify the feasibility of these input modalities in a series of user studies, which suggest that AuraSense can be low latency and robust across both users and environments.&nbsp;</p><p><a href="/s/aurasense.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_917">


          <div class="image" id="yui_3_17_2_1_1618546122048_916">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/gZGqkpuwzrA?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_924"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535027182-NFKODA7KFJ7TBI9WPXNC/ke17ZwdGBToddI8pDm48kEDgYR9XbNp02iaWmThWwtpZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwmOgeyy797S7k3Uv0CFPgtkH9JB6gMToD1APwWP_R3ECMa-tkANRNErJV8Pek5-Ak/thumbnail.jpg" data-image-dimensions="668x676" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Zhou, J., Yang, C., Laput, G., Sample, A. and Harrison, C. 2016. AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing. To appear in Proceedings of the 29th Annual ACM Symposium on User interface Software and Technology (Tokyo, Japan, October 16 - 19, 2016). UIST '16. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/tomo2/" id="yui_3_17_2_1_1618546122048_947">

        <div class="project-meta">
          <h2 class="project-title">Tomo 2 (2016)</h2>
          <div class="project-description"><p>Electrical Impedance Tomography (EIT) can be used to detect hand gestures using an instrumented smartwatch (see Tomo), demonstrating great promise for non-invasive, high accuracy recognition of gestures for interactive control. In Tomo 2, we introduce a new system that offers improved sampling speed and resolution. This, in turn, enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation – we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.</p><p><a href="/s/Tomo2.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_946">


          <div class="image" id="yui_3_17_2_1_1618546122048_945">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/6a8q7HON4_c?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_953"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535884245-NUWZFFRB2WCLVKU52X90/ke17ZwdGBToddI8pDm48kH57sgD1tvYEeMmeWW5KQ1JZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVGP3PEk-4tU-lRdiu5v5GR_LnhVR9M5gwz1lz67kkv7Mxur-lC0WofN0YB1wFg-ZW0/Tomo2.png" data-image-dimensions="398x398" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Zhang, Y., Xiao, R., and Harrison, C. 2016. Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography. To appear in Proceedings of the 29th Annual ACM Symposium on User interface Software and Technology (Tokyo, Japan, October 16 - 19, 2016). UIST '16. ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/skintrack-2016/" id="yui_3_17_2_1_1618546122048_976">

        <div class="project-meta">
          <h2 class="project-title">SkinTrack (2016)</h2>
          <div class="project-description"><p>SkinTrack enables continuous touch tracking on the skin. It consists of a ring, which emits a continuous high frequency AC signal, and a sensing wristband with multiple electrodes. SkinTrack measures phase differences to compute a 2D finger touch coordinate. Our approach is compact, non-invasive, low-cost and low-powered. We envision the technology being integrated into future smartwatches, supporting rich touch interactions beyond the confines of the small touchscreen.</p><p><a href="/s/skintrack.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_975">


          <div class="image" id="yui_3_17_2_1_1618546122048_974">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/9hu8MNuvCHE?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_982"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1462496732808-SQHIA1XFPRLPS4HIR0BR/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0foACs49-HBkG_F4C3fTziO-wh7WAJ8x5TfmquXBnDIxLo8HbKjlB3UeKJEwFtT03xOqpeNLcJ80NK65_fV7S1UdmYXQF-zPI03SGB9DVl4jXb1vAd_BNBc77xmQwsy4hC1KcsXV-dqmPmf9JV4qUaPg/Thumbnail.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Zhang, Y., Zhou, J., Laput, G., Harrison, C. SkinTrack: Using the Body as an Electrical Waveguide for Continuous Finger Tracking on the SkinIn Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/sweepsense/" id="yui_3_17_2_1_1618546122048_1005">

        <div class="project-meta">
          <h2 class="project-title">SweepSense (2016)</h2>
          <div class="project-description"><p>We use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos.</p><p><a href="http://www.gierad.com/">Download Paper</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1004">


          <div class="image" id="yui_3_17_2_1_1618546122048_1003">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/iPRdL9K-ZQQ?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>
" data-provider-name="" id="yui_3_17_2_1_1618546122048_1011"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1462496405974-MZF18WKLQR1ENSTWX4BF/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0faShnVfr-ySw9qgw5FxrvM5QQqCM9w82lPHqXEyFQjFaLYXVbONl_hZyFpVyn4qZhOqpeNLcJ80NK65_fV7S1UaqrTmIxJWevOIsd2YanSB-sSVoyS0tOo2qZJuaV_wa8BBBFbt9YekZp4EE0dHePbg/02_Right_Out.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Xiao, A.C., Harrison, C.&nbsp;SweepSense: Ad-Hoc Configuration Sensing Using Swept-Frequency Ultrasonics. In Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI ‘16). ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/fingerpose/" id="yui_3_17_2_1_1618546122048_1034">

        <div class="project-meta">
          <h2 class="project-title">FingerPose (2015)</h2>
          <div class="project-description"><p>A new method that estimates a finger’s angle relative to the screen. &nbsp;Our approach works in tandem with conventional multitouch finger tracking, offering two additional analog degrees of freedom for a single touch point. We prototyped our solution on two platforms—&nbsp;a smartphone and smartwatch—each fully self-contained and operating in real-time.&nbsp;</p><p><a href="http://chrisharrison.net/projects/3dfingerangle/Qeexo3DFingerAngle.pdf">Download Paper</a><br><a target="_blank" href="http://chrisharrison.net/index.php/Research/3DFingerAngle">More Info</a></p><p><strong>Research conducted at:</strong><br><a target="_blank" href="http://www.qeexo.com">Qeexo</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1033">


          <div class="image" id="yui_3_17_2_1_1618546122048_1032">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/hLYBEBJHFYY?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1040"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895629619-YX0OWEMUW6UEKV98M64P/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Xiao, R. Schwarz, J. and Harrison, C. Estimating 3D Finger Angle on Commodity Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ‘15). ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/capauth/" id="yui_3_17_2_1_1618546122048_1063">

        <div class="project-meta">
          <h2 class="project-title">CapAuth (2015)</h2>
          <div class="project-description"><p>A technique that uses existing, low-level touchscreen data, combined with machine learning classifiers, to provide real-time authentication and even identification of users. Our user study demonstrates twenty-participant authentication accuracies of 99.6%. For twenty-user identification, our software achieved 94.0% accuracy and 98.2% on groups of four, simulating family use.</p><p><a href="http://chrisharrison.net/projects/capauth/CapAuth.pdf">Download Paper</a><br><a target="_blank" href="http://chrisharrison.net/index.php/Research/CapAuth">More Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1062">


          <div class="image" id="yui_3_17_2_1_1618546122048_1061">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/jGH78KWH_yM?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1069"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895951793-Y8KAL1K9NPV7OVS5YILH/ke17ZwdGBToddI8pDm48kJaY95dotR99Y3f9Nm8ToPF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCYWGxfdB_uf1_ERfebHZ44G_2GqWQDW_nyQTS8zPi5D2Ogm_Ha3R8LNAReSVjEjA/image-asset.jpeg" data-image-dimensions="3957x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Guo, A., Xiao, R. and Harrison, C. 2015. CapAuth: Identifying and Differentiating User Handprints on Commodity Capacitive Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (Madeira, Portugal, November 15 - 18, 2015). ITS ‘15. ACM, New York, NY. 59-62.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/gazegesture/" id="yui_3_17_2_1_1618546122048_1092">

        <div class="project-meta">
          <h2 class="project-title">Gaze+Gesture (2015)</h2>
          <div class="project-description"><p>By fusing gaze and gesture into a unified and fluid interaction modality, we can enable rapid, precise and expressive free-space interactions that mirror natural use. Although both approaches are independently poor for pointing tasks, combining them can achieve pointing performance superior to either method alone. This opens new interaction opportunities for gaze and gesture systems alike.</p><p><a href="http://chrisharrison.net/projects/gazegesture/GazePlusGesture.pdf">Download Paper</a><br><a target="_blank" href="http://chrisharrison.net/index.php/Research/GazePlusGesture">More Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1091">


          <div class="image" id="yui_3_17_2_1_1618546122048_1090">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/IjV2D69Gpio?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1098"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896165931-F4O6HUDKKIBCTO0O30SB/ke17ZwdGBToddI8pDm48kB6N0s8PWtX2k_eW8krg04V7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1URWK2DJDpV27WG7FD5VZsfFVodF6E_6KI51EW1dNf095hdyjf10zfCEVHp52s13p8g/image-asset.jpeg" data-image-dimensions="1800x1200" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Chatterjee, I., Xiao, R. and Harrison, C. 2015. Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions. In Proceedings of the 17th ACM International Conference on Multimodal Interaction (Seattle, Washington, November 9 - 13, 2015). ICMI '15. ACM, New York, NY. 131-138.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/emsense/" id="yui_3_17_2_1_1618546122048_1121">

        <div class="project-meta">
          <h2 class="project-title">EM-Sense (2015)</h2>
          <div class="project-description"><p>A sensing technology that allows a smartwatch to know what object the user is touching. &nbsp;When the user operates an electrical or electro-mechanical object, the electro-magnetic signals (EM) propagate through the user. &nbsp;These characteristic signals flow through the user and detected by the watch, which can be used for on-touch object detection.</p><p><a href="http://www.disneyresearch.com/wp-content/uploads/EMSense-Recognizing-Handled-Uninstrumented-Electro-Mechanical-Objects-Using-Software-Defined-Radio-Paper.pdf">Download Paper</a><br><a href="http://gierad.com/projects/emsense">More Info on Gierad's Webpage</a></p><p><strong>In collaboration with:</strong><br>Disney Research</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1120">


          <div class="image" id="yui_3_17_2_1_1618546122048_1119">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/fpKDNle6ia4?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1127"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896338545-QUXSR8S65XNBICRAUBZP/ke17ZwdGBToddI8pDm48kGwvvILFRU8TVLtQPFuX3HB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UYdWN69ZQzgEqKIIOw4FvIvDIvENyykEVQO8nSDRMvqgBBBFbt9YekZp4EE0dHePbg/image-asset.jpeg" data-image-dimensions="2136x1536" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Yang, C. Xiao, R, Sample, A. and Harrison, C. 2015. EM-Sense: Touch Recognition of Uninstrumented, Electrical and Electromechanical Objects. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (Charlotte, North Carolina, November 8 - 11, 2015). UIST '15. ACM, New York, NY. 157-166.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/tomo/" id="yui_3_17_2_1_1618546122048_1150">

        <div class="project-meta">
          <h2 class="project-title">Tomo (2015)</h2>
          <div class="project-description"><p>Tomo is a wearable, low-cost system using Electrical Impedance Tomography (EIT) to recover the interior impedance geometry of a user’s arm. &nbsp;We ultimately envision this technique being integrated into future smartwatches, allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens.&nbsp;</p><p><a href="http://www.yang-zhang.me/paper/Tomo.pdf">Download Paper</a><br><a href="http://chrisharrison.net/index.php/Research/Tomo">More Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1149">


          <div class="image" id="yui_3_17_2_1_1618546122048_1148">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/N9c4hINa2Bk?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1156"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896670661-4OBSKMYGN5MK94C0VMQ6/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Zhang, Y. and Harrison, C. 2015. Tomo: Wearable, Low-Cost, Electrical Impedance Tomography for Hand Gesture Recognition. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (Charlotte, North Carolina, November 8 - 11, 2015). UIST '15. ACM, New York, NY. 167-173.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/furbrication/" id="yui_3_17_2_1_1618546122048_1179">

        <div class="project-meta">
          <h2 class="project-title">3D-Printed Hair (2015)</h2>
          <div class="project-description"><p>A technique for 3D printing hair, fibers and bristles, by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling. This technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware</p><p><a href="http://gierad.com/assets/3dprintedhair/furbrication.pdf">Download Paper</a><br><a href="http://www.gierad.com/projects/furbrication/">More Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1178">


          <div class="image" id="yui_3_17_2_1_1618546122048_1177">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/FB9YZLO0-AI?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1185"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447787915596-C2IPJXYI3WJRO2SNRA9I/ke17ZwdGBToddI8pDm48kCFTy3UGnhJ-fWvK5PBTMNx7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UQAK6Hrx5oK4v7EgPqT2OFRSRLMHaWtK29l-R4I7CH9pArRUwoSl4wuUs73k4k9WwA/image-asset.jpeg" data-image-dimensions="2000x1334" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Chen, X. and Harrison, C. 3D Printed Hair: Fused Deposition Modeling of Soft Strands, Fibers and Bristles. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (UIST '15). ACM, New York, NY.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/zensors/" id="yui_3_17_2_1_1618546122048_1208">

        <div class="project-meta">
          <h2 class="project-title">Zensors (2015)</h2>
          <div class="project-description"><p>Zensors is a new sensing approach that fuses real-time human intelligence from online crowd workers with automatic approaches to provide robust, adaptive, and readily deployable intelligent sensors. With Zensors, users can go from question to live sensor feed in less than 60 seconds. Through our API, Zensors can enable a variety of rich end-user applications and moves us closer to the vision of responsive, intelligent environments.&nbsp;</p><p><a href="http://www.gierad.com/assets/zensors/zensors.pdf">Download Paper</a><br><a href="http://www.gierad.com/projects/zensors/">More Info at Gierad's Webpage</a></p><p><strong>Sponsor:</strong><br>Yahoo! InMind</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1207">


          <div class="image" id="yui_3_17_2_1_1618546122048_1206">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/VVP9emuFsQI?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1214"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897310800-SEO8U7RHY97Y1FBPO6S7/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Lasecki, W., Wiese, J., Xiao, R., Bigham, J. and Harrison, C. 2015. Zensors: Adaptive, Rapidly Deployable, Human-Intelligent Sensor Feeds. In Proceedings of the 33nd Annual SIGCHI Conference on Human Factors in Computing Systems (CHI '15). 1935-1944.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/new-gallery/" id="yui_3_17_2_1_1618546122048_1237">

        <div class="project-meta">
          <h2 class="project-title">Acoustruments (2015)</h2>
          <div class="project-description"><p>Acoustruments are low-cost, passive, and powerless mechanisms, made from plastic, that can bring tangible functionality to handheld devices. The operational principles were inspired by wind instruments, which produce expressive musical output despite being simple in physical design. Through a structured exploration, we built an expansive vocabulary of design primitives, providing building blocks for the construction of tangible interfaces utilizing smartphonesʼ existing audio functionality (the speaker and microphone).&nbsp;</p><p><a href="http://www.gierad.com/assets/acoustruments/acoustruments.pdf">Download Paper</a><br><a href="http://www.gierad.com/projects/acoustruments/">More Info at Gierad's Webpage</a></p><p><strong>In collaboration with:</strong><br>Disney Research</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1236">


          <div class="image" id="yui_3_17_2_1_1618546122048_1235">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/llOKDcr1gsY?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1243"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897463888-TR9WC8MO62336H90GW4J/ke17ZwdGBToddI8pDm48kOggE0Ch6pMGalwtLMqzsSB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1Ufo5RWkg_J4of0jUNHaDHx6pZKBvpVYzidBWCapg0tuoMuEaB2HPGSYDV-11UTcW2g/image-asset.jpeg" data-image-dimensions="1920x1280" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Brockmeyer, E., Hudson, S. and Harrison, C. 2015. Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices. In Proceedings of the 33nd Annual SIGCHI Conference on Human Factors in Computing Systems (Seoul, Korea, April 18 - 23, 2015). CHI '15. ACM, New York, NY. 2161-2170.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/skinbuttons/" id="yui_3_17_2_1_1618546122048_1266">

        <div class="project-meta">
          <h2 class="project-title">Skin Buttons (2014)</h2>
          <div class="project-description"><p>Tiny projectors integrated into the smartwatch to render icons on the user’s skin. These icons can be made touch sensitive, significantly expanding the interactive region without increasing device size. Through a series of experiments, we show that these “skin buttons” can have high touch accuracy and recognizability, while being low cost and power-efficient.</p><p><a href="http://www.gierad.com/assets/skinbuttons/skinbuttons.pdf">Download Paper</a><br><a href="http://www.gierad.com/projects/skinbuttons/">More Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1265">


          <div class="image" id="yui_3_17_2_1_1618546122048_1264">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/OJVBuI3LFqU?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1272"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897780089-MS9T8XA80014SCZHQVE9/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Laput, G., Xiao, R., Chen, X., Hudson, S. and Harrison, C. 2014. Skin Buttons: Cheap, Small, Low-Power and Clickable Fixed-Icon Laser Projections. In Proceedings of the 27th Annual ACM Symposium on User interface Software and Technology (Honolulu, Hawaii, October 5 - 8, 2014). UIST '14. ACM, New York, NY. 389-394.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/airplustouch/" id="yui_3_17_2_1_1618546122048_1295">

        <div class="project-meta">
          <h2 class="project-title">Air+Touch (2014)</h2>
          <div class="project-description"><p>Air+Touch is a new class of interactions that interweave touch events with in-air gestures, offering a unified input modality with expressiveness greater than each input modality alone. We demonstrate how air and touch are highly complementary: touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events.&nbsp;</p><p><a href="http://chrisharrison.net/projects/airplustouch/airplustouchCMU.pdf">Download Paper</a><br><a href="http://chrisharrison.net/index.php/Research/AirPlusTouch">More </a><a href="#">Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1294">


          <div class="image" id="yui_3_17_2_1_1618546122048_1293">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe mozallowfullscreen allowfullscreen src=&quot;https://player.vimeo.com/video/92972949?wmode=opaque&quot; webkitallowfullscreen frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1301"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1452484860855-717VCTX14QDX5CX83ZO1/ke17ZwdGBToddI8pDm48kA9rhCjhJUYcQpKsBaLXN1ZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIye_uGeP4isZUnhF2J4BSLX0iSmbQA7pLf20f1CNe8SkKMshLAGzx4R3EDFOm1kBS/image-asset.jpeg" data-image-dimensions="750x500" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Chen, X., Schwarz, J. Harrison, C., Mankoff, J. and Hudson, S. 2014. Air+Touch: Interweaving Touch &amp; In-Air Gestures. In Proceedings of the 27th Annual ACM Symposium on User interface Software and Technology (Honolulu, Hawaii, October 5 - 8, 2014). UIST '14. ACM, New York, NY. 519-525.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/teslatouch/" id="yui_3_17_2_1_1618546122048_1323">

        <div class="project-meta">
          <h2 class="project-title">Toffee (2014)</h2>
          <div class="project-description"><p>Toffee is a sensing approach that extends touch interaction beyond the small confines of a mobile device and onto ad hoc adjacent surfaces, most notably tabletops. This is achieved using a novel application of acoustic time differences of arrival (TDOA) correlation. This enables radial interactions in an area many times larger than a mobile device.</p><p><a href="http://chrisharrison.net/projects/toffee/ToffeeCMU.pdf">Download Paper</a><br><a href="http://chrisharrison.net/index.php/Research/Toffee">More I</a><a href="http://chrisharrison.net/index.php/Research/Toffee">nfo</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1322">


          <div class="image" id="yui_3_17_2_1_1618546122048_1321">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/_Edph1kb4ck?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1329"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898632072-D28A6U7ETKZ99OE0MFYQ/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Xiao, R., Lew, G., Marsanico, J., Hariharan, D., Hudson, S., and Harrison, C. 2014. Toffee: Enabling Ad Hoc, Around-Device Interaction with Acoustic Time-of-Arrival Correlation. In Proceedings of the 16th International Conference on Human-Computer Interaction with Mobile Devices and Services (Toronto, Canada, September 23 - 26, 2014). MobileHCI ’14. ACM, New York, NY. 67-76.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/touchtools/" id="yui_3_17_2_1_1618546122048_1352">

        <div class="project-meta">
          <h2 class="project-title">TouchTools (2014)</h2>
          <div class="project-description"><p>We propose that touch gesture design be inspired by the manipulation of physical tools from the real world. In this way, we can leverage user familiarity and fluency with such tools to build a rich set of gestures for touch interaction. With only a few minutes of training on a proof-of-concept system, users were able to summon a variety of virtual tools by replicating their corresponding real-world grasps.&nbsp;</p><p><a href="http://chrisharrison.net/projects/touchtools/TouchTools.pdf">Download Paper</a><br><a href="http://chrisharrison.net/index.php/Research/Touchtools">More Info</a></p><p><strong>Commercialized by:</strong><br>Qeexo</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1351">


          <div class="image" id="yui_3_17_2_1_1618546122048_1350">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/N8s8NJf34fM?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1358"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447789812527-ZD2ZQCJRA87VSS8BY5Q4/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Harrison, C., Xiao, R., Schwarz, J., and Hudson, S. TouchTools: Leveraging Familiarity and Skill with Physical Tools to Augment Touch Interaction. In Proceedings of the 32nd Annual SIGCHI Conference on Human Factors in Computing Systems (Toronto, Canada, April 26 - May 1, 2014). CHI '14. ACM, New York, NY. 2913-2916.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/smartwatch-5dof/" id="yui_3_17_2_1_1618546122048_1381">

        <div class="project-meta">
          <h2 class="project-title">Smartwatch 5DOF (2014)</h2>
          <div class="project-description"><p>We propose using the face of a smartwatch as a multi-degree-of-freedom mechanical interface. This enables rich interaction without occluding the screen with fingers, and can operate in concert with touch interaction and physical buttons. We developed a series of example applications, many of which are cumbersome – or even impossible – on today’s smartwatch devices.</p><p><a href="http://www.gierad.com/assets/smartwatchface/smartwatchface.pdf">Download Paper</a><br><a href="http://www.gierad.com/projects/smartwatch5dof/">More Info</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1380">


          <div class="image" id="yui_3_17_2_1_1618546122048_1379">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/rLQtqTpZBOU?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1387"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450899044774-47PZMXK1VDYRATHDU2HB/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Xiao, R., Laput, G., and Harrison, C. Expanding the Input Expressivity of Smartwatches with Mechanical Pan, Twist, Tilt and Click. In Proceedings of 32nd Annual SIGCHI Conference on Human Factors in Computing Systems (Toronto, Canada, April 26 - May 1, 2014). CHI '14. ACM, New York, NY. 193-196.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/tapsense/" id="yui_3_17_2_1_1618546122048_1410">

        <div class="project-meta">
          <h2 class="project-title">TapSense (2011)</h2>
          <div class="project-description"><p>TapSense enables touchscreens to know how users are touching the screen - with finger tips, knuckles and nails, or even a passive stylus. TapSense moves beyond just counting the number of fingers on the screen, revolutionizing the way we interact with touch-enabled devices. By distinguishing between different parts of the hand, TapSense takes the pain out of performing these actions, making mobile devices faster and easier to use than ever.</p><p><a href="http://chrisharrison.net/projects/tapsense/tapsense.pdf">Download Paper</a><br><a href="http://chrisharrison.net/index.php/Research/TapSense">More Info</a></p><p><strong>Commercialized by:</strong><br><a target="_blank" href="http://www.qeexo.com">Qeexo</a></p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1409">


          <div class="image" id="yui_3_17_2_1_1618546122048_1408">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/-oN96cucBr4?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1416"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790172793-ESDG2ZRE268GNJ1RBK42/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Harrison, C., Schwarz, J. and Hudson S. E. 2011. TapSense: Enhancing Finger Interaction on Touch Surfaces. In Proceedings of the 24th Annual ACM Symposium on User interface Software and Technology. UIST '11. ACM, New York, NY. 627-636.</p></span>
            </div>
          </div>

        </div>

      </div>

      <div class="project gallery-project" data-url="/teslatouch-1/" id="yui_3_17_2_1_1618546122048_1439">

        <div class="project-meta">
          <h2 class="project-title">TeslaTouch (2010)</h2>
          <div class="project-description"><p>TeslaTouch brings rich, dynamic physical feedback to otherwise flat, featureless touchscreens. The technology is based on the electrovibration principle, which can programmatically vary the electrostatic friction between fingers and a touch panel. When combined with an interactive graphical display, this approach enables touch experiences with rich textures and physical affordances.</p><p><a href="http://chrisharrison.net/projects/teslatouch/teslatouchUIST2010.pdf">Download Paper</a><br><a href="http://www.disneyresearch.com/project/teslatouch/">More Info at Disney Research</a></p><p><strong>In collaboration with:</strong><br>Disney Research</p></div>
        </div>
        <div class="image-list" id="yui_3_17_2_1_1618546122048_1438">


          <div class="image" id="yui_3_17_2_1_1618546122048_1437">

            <div class="sqs-video-wrapper video-none" data-load="false" data-html="<iframe allowfullscreen src=&quot;https://www.youtube.com/embed/3l3MDNZk-3I?wmode=opaque&quot; frameborder=&quot;0&quot;></iframe>" data-provider-name="" id="yui_3_17_2_1_1618546122048_1445"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790296744-4W7UH8NE598OATKOL7LZ/ke17ZwdGBToddI8pDm48kBfJC82zr283-cptPlVKgQIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcFt3EflJyNsqD_uNZ9Iq801j8fuIfQPk1oLr34Z1h6J52ksZGCORNbuHLRu5woUzZ/image-asset.jpeg" data-image-dimensions="1000x350" data-image-focal-point="0.5,0.5"><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">

              <span class="image-desc"><p>Bau, O., Poupyrev, I., Israr, A., and Harrison, C. 2010. TeslaTouch: Electrovibration for Touch Surfaces. In Proceedings of the 23rd Annual ACM Symposium on User interface Software and Technology (New York, New York, October 3 - 6, 2010). UIST '10. ACM, New York, NY. 283-292.</p></span>
            </div>
          </div>

        </div>

      </div>



  <div class="project-controls">
    <div>
      <div id="projectNav"><span class="prev-project">prev</span> / <span class="next-project">next</span></div>
      <a href="#">Back to Research</a>
    </div>
  </div>

</div>



<div id="projectThumbs">
  <div class="wrapper">


        <a class="project " href="/bodyslam/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605715765776-REYYSXAFYBAR4IRDDN2Q/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/image-asset.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605715765776-REYYSXAFYBAR4IRDDN2Q/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/image-asset.png" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="BodySLAM: Opportunistic User Digitization in Multi-User AR/VR Experiences" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605715765776-REYYSXAFYBAR4IRDDN2Q/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/image-asset.png?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605715765776-REYYSXAFYBAR4IRDDN2Q/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/image-asset.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">BodySLAM (2020)</div>
          </div>
        </a>

        <a class="project " href="/dov/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605716252964-B6G0R11K5W9XUXFK2MTA/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605716252964-B6G0R11K5W9XUXFK2MTA/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg" data-image-dimensions="1920x1080" data-image-focal-point="0.24577910198107783,0.6429169379592596" data-load="false" data-parent-ratio="1.0" alt="Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605716252964-B6G0R11K5W9XUXFK2MTA/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg?format=500w" style="font-size: 0px; left: 0px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1605716252964-B6G0R11K5W9XUXFK2MTA/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Direction of Voice (2020)</div>
          </div>
        </a>

        <a class="project " href="/vibrocomm/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690794839-X44EJ1UUEH9665M2TGZO/ke17ZwdGBToddI8pDm48kGVW7U5QjGewqrYX_NVbpUUUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dlof2yW2bIZNdXF7rHshQKKruoSkZp9-5cD4CB6JCsfMZtJ3qR9G2BYeA0wOAaeYNg/image.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690794839-X44EJ1UUEH9665M2TGZO/ke17ZwdGBToddI8pDm48kGVW7U5QjGewqrYX_NVbpUUUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dlof2yW2bIZNdXF7rHshQKKruoSkZp9-5cD4CB6JCsfMZtJ3qR9G2BYeA0wOAaeYNg/image.jpg" data-image-dimensions="1730x973" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="VibroComm: Using Commodity Gyroscopes for Vibroacoustic Data Reception" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690794839-X44EJ1UUEH9665M2TGZO/ke17ZwdGBToddI8pDm48kGVW7U5QjGewqrYX_NVbpUUUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dlof2yW2bIZNdXF7rHshQKKruoSkZp9-5cD4CB6JCsfMZtJ3qR9G2BYeA0wOAaeYNg/image.jpg?format=500w" style="font-size: 0px; left: -72.3546px; top: 0px; width: 330.709px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690794839-X44EJ1UUEH9665M2TGZO/ke17ZwdGBToddI8pDm48kGVW7U5QjGewqrYX_NVbpUUUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dlof2yW2bIZNdXF7rHshQKKruoSkZp9-5cD4CB6JCsfMZtJ3qR9G2BYeA0wOAaeYNg/image.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">VibroComm (2020)</div>
          </div>
        </a>

        <a class="project " href="/listen-learner/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690110355-ZHKFBOAZGLWZFR62XS8C/ke17ZwdGBToddI8pDm48kLIgtPySYY_xO2fZKaV4Az0UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dvczmhjPcXeZMtv_SqJObyBnXfo-USe_5Tl-j4vPa1xPm4bjm9DAHF2kOsIZRJKXnA/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690110355-ZHKFBOAZGLWZFR62XS8C/ke17ZwdGBToddI8pDm48kLIgtPySYY_xO2fZKaV4Az0UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dvczmhjPcXeZMtv_SqJObyBnXfo-USe_5Tl-j4vPa1xPm4bjm9DAHF2kOsIZRJKXnA/image-asset.jpeg" data-image-dimensions="1634x919" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="Listen Learner: Automatic Class Discovery &amp; One-Shot Interactions for Acoustic Activity Recognition" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690110355-ZHKFBOAZGLWZFR62XS8C/ke17ZwdGBToddI8pDm48kLIgtPySYY_xO2fZKaV4Az0UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dvczmhjPcXeZMtv_SqJObyBnXfo-USe_5Tl-j4vPa1xPm4bjm9DAHF2kOsIZRJKXnA/image-asset.jpeg?format=500w" style="font-size: 0px; left: -72.3558px; top: 0px; width: 330.712px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1602690110355-ZHKFBOAZGLWZFR62XS8C/ke17ZwdGBToddI8pDm48kLIgtPySYY_xO2fZKaV4Az0UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dvczmhjPcXeZMtv_SqJObyBnXfo-USe_5Tl-j4vPa1xPm4bjm9DAHF2kOsIZRJKXnA/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Listen Learner (2020)</div>
          </div>
        </a>

        <a class="project " href="/wireality-2020/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1587750753982-JW3NY0OIJG47S50RVUHI/ke17ZwdGBToddI8pDm48kECGXzeLompxCB9XLwW2LBh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UW7TDhHdMmhRWA-_rDPuz1BDSURgyO_REA5DNXnKX-BkP7cJNZlDXbgJNE9ef52e8w/WirealityHero1.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1587750753982-JW3NY0OIJG47S50RVUHI/ke17ZwdGBToddI8pDm48kECGXzeLompxCB9XLwW2LBh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UW7TDhHdMmhRWA-_rDPuz1BDSURgyO_REA5DNXnKX-BkP7cJNZlDXbgJNE9ef52e8w/WirealityHero1.png" data-image-dimensions="1563x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="WirealityHero1.png" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1587750753982-JW3NY0OIJG47S50RVUHI/ke17ZwdGBToddI8pDm48kECGXzeLompxCB9XLwW2LBh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UW7TDhHdMmhRWA-_rDPuz1BDSURgyO_REA5DNXnKX-BkP7cJNZlDXbgJNE9ef52e8w/WirealityHero1.png?format=300w" style="font-size: 0px; left: -41.5917px; top: 0px; width: 269.183px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1587750753982-JW3NY0OIJG47S50RVUHI/ke17ZwdGBToddI8pDm48kECGXzeLompxCB9XLwW2LBh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UW7TDhHdMmhRWA-_rDPuz1BDSURgyO_REA5DNXnKX-BkP7cJNZlDXbgJNE9ef52e8w/WirealityHero1.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Wireality (2020)</div>
          </div>
        </a>

        <a class="project " href="/worldgaze-2020/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1586890560674-REL66P56T4FN3V2NMR5F/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/mayer2020worldgaze.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1586890560674-REL66P56T4FN3V2NMR5F/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/mayer2020worldgaze.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="mayer2020worldgaze.jpg" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1586890560674-REL66P56T4FN3V2NMR5F/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/mayer2020worldgaze.jpg?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1586890560674-REL66P56T4FN3V2NMR5F/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/mayer2020worldgaze.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">WorldGaze (2020)</div>
          </div>
        </a>

        <a class="project " href="/lightanchors-2019/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903661786-1837K7W06KT48R6S9M0S/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/lightanchors.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903661786-1837K7W06KT48R6S9M0S/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/lightanchors.png" data-image-dimensions="720x720" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="LightAnchors: Appropriating Point Lights for Spatially-Anchored Augmented Reality Interfaces" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903661786-1837K7W06KT48R6S9M0S/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/lightanchors.png?format=300w" style="font-size: 0px; left: -1.42109e-14px; top: -1.42109e-14px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903661786-1837K7W06KT48R6S9M0S/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/lightanchors.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">LightAnchors (2019)</div>
          </div>
        </a>

        <a class="project " href="/sozu-2019/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572881691410-P7RIBS4PGCUQQWYG7LFF/ke17ZwdGBToddI8pDm48kBIulzeX5j9NT9xsNh53_SlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIPNYiFL7BnXyjRMbW9h8dM04P3OJO1lsaFgDfvoR4Ro8KMshLAGzx4R3EDFOm1kBS/thumbnail.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572881691410-P7RIBS4PGCUQQWYG7LFF/ke17ZwdGBToddI8pDm48kBIulzeX5j9NT9xsNh53_SlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIPNYiFL7BnXyjRMbW9h8dM04P3OJO1lsaFgDfvoR4Ro8KMshLAGzx4R3EDFOm1kBS/thumbnail.jpg" data-image-dimensions="811x811" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="Sozu: Self-Power Radio Tags for Building-Scale Activity Sensing" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572881691410-P7RIBS4PGCUQQWYG7LFF/ke17ZwdGBToddI8pDm48kBIulzeX5j9NT9xsNh53_SlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIPNYiFL7BnXyjRMbW9h8dM04P3OJO1lsaFgDfvoR4Ro8KMshLAGzx4R3EDFOm1kBS/thumbnail.jpg?format=300w" style="font-size: 0px; left: 0px; top: 0px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572881691410-P7RIBS4PGCUQQWYG7LFF/ke17ZwdGBToddI8pDm48kBIulzeX5j9NT9xsNh53_SlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIPNYiFL7BnXyjRMbW9h8dM04P3OJO1lsaFgDfvoR4Ro8KMshLAGzx4R3EDFOm1kBS/thumbnail.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Sozu (2019)</div>
          </div>
        </a>

        <a class="project " href="/actitouch-2019/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572883502552-TS0H8518ZXGHNSM5LB05/ke17ZwdGBToddI8pDm48kJxzbK9g66s9755W1mft6Jp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmp0iglcLYVjOea_4STb6GncJu24-mjs-PD1sbKDlJam1ANT7JgEFYgkN6X0N795Ud/thumbnail.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572883502552-TS0H8518ZXGHNSM5LB05/ke17ZwdGBToddI8pDm48kJxzbK9g66s9755W1mft6Jp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmp0iglcLYVjOea_4STb6GncJu24-mjs-PD1sbKDlJam1ANT7JgEFYgkN6X0N795Ud/thumbnail.png" data-image-dimensions="1095x1096" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="ActiTouch: Robust Touch Detection for On-Skin AR/VR Interactions" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572883502552-TS0H8518ZXGHNSM5LB05/ke17ZwdGBToddI8pDm48kJxzbK9g66s9755W1mft6Jp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmp0iglcLYVjOea_4STb6GncJu24-mjs-PD1sbKDlJam1ANT7JgEFYgkN6X0N795Ud/thumbnail.png?format=300w" style="font-size: 0px; left: 0px; top: -0.0849315px; width: 186px; height: 186.17px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572883502552-TS0H8518ZXGHNSM5LB05/ke17ZwdGBToddI8pDm48kJxzbK9g66s9755W1mft6Jp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmp0iglcLYVjOea_4STb6GncJu24-mjs-PD1sbKDlJam1ANT7JgEFYgkN6X0N795Ud/thumbnail.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">ActiTouch (2019)</div>
          </div>
        </a>

        <a class="project " href="/mecap-2019/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903441987-YZOCGTNYJVUDA6XNEUNK/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/mecap.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903441987-YZOCGTNYJVUDA6XNEUNK/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/mecap.png" data-image-dimensions="720x720" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="MeCap: Whole-Body Digitization for Low-Cost VR/AR Headsets" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903441987-YZOCGTNYJVUDA6XNEUNK/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/mecap.png?format=300w" style="font-size: 0px; left: -1.42109e-14px; top: -1.42109e-14px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903441987-YZOCGTNYJVUDA6XNEUNK/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/mecap.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">MeCap (2019)</div>
          </div>
        </a>

        <a class="project " href="/surfacesight/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557219886228-2MC14FL20UOS5ZZOA07T/ke17ZwdGBToddI8pDm48kBL7y0DpH_e0bbX7enFn0u57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UT4ke-2faLbDJ2EQYXZquaaYAmln24LeHvojPJ-RAjfURBpFeAJxHLqEa5Otf4mhGw/thumb.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557219886228-2MC14FL20UOS5ZZOA07T/ke17ZwdGBToddI8pDm48kBL7y0DpH_e0bbX7enFn0u57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UT4ke-2faLbDJ2EQYXZquaaYAmln24LeHvojPJ-RAjfURBpFeAJxHLqEa5Otf4mhGw/thumb.jpg" data-image-dimensions="1920x1920" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557219886228-2MC14FL20UOS5ZZOA07T/ke17ZwdGBToddI8pDm48kBL7y0DpH_e0bbX7enFn0u57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UT4ke-2faLbDJ2EQYXZquaaYAmln24LeHvojPJ-RAjfURBpFeAJxHLqEa5Otf4mhGw/thumb.jpg?format=300w" style="font-size: 0px; left: 0px; top: 0px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557219886228-2MC14FL20UOS5ZZOA07T/ke17ZwdGBToddI8pDm48kBL7y0DpH_e0bbX7enFn0u57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UT4ke-2faLbDJ2EQYXZquaaYAmln24LeHvojPJ-RAjfURBpFeAJxHLqEa5Otf4mhGw/thumb.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">SurfaceSight (2019)</div>
          </div>
        </a>

        <a class="project " href="/edusense-2019/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903837236-TV41YJ6QIT1OFGNJ7F3M/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903837236-TV41YJ6QIT1OFGNJ7F3M/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png" data-image-dimensions="720x720" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="EduSense: Practical Classroom Sensing at Scale" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903837236-TV41YJ6QIT1OFGNJ7F3M/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png?format=300w" style="font-size: 0px; left: -1.42109e-14px; top: -1.42109e-14px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1572903837236-TV41YJ6QIT1OFGNJ7F3M/ke17ZwdGBToddI8pDm48kMRyhYDgPk9y7GMUxyVGe-RZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzoPHppnmNwl3TJcatNy7Guep7qP-wNjXM-6rVCPo3NX5PuHDDAzCDFVJ9rfhLFRLA/edusense.png?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">EduSense (2019)</div>
          </div>
        </a>

        <a class="project " href="/handactivities/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557218838661-KUUDGYT8XLPXAJ70MZWZ/ke17ZwdGBToddI8pDm48kKY-G6HndK7v2FlTyTIJQI57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UWBFTp-jyz-u-yJnrxahDVdFAjR2sOZTixm23ygffcf2JQf5RnFi622klzb8JVxQkQ/thumb.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557218838661-KUUDGYT8XLPXAJ70MZWZ/ke17ZwdGBToddI8pDm48kKY-G6HndK7v2FlTyTIJQI57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UWBFTp-jyz-u-yJnrxahDVdFAjR2sOZTixm23ygffcf2JQf5RnFi622klzb8JVxQkQ/thumb.jpg" data-image-dimensions="1814x1814" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="Sensing Fine-Grained Hand Activity with Smartwatches" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557218838661-KUUDGYT8XLPXAJ70MZWZ/ke17ZwdGBToddI8pDm48kKY-G6HndK7v2FlTyTIJQI57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UWBFTp-jyz-u-yJnrxahDVdFAjR2sOZTixm23ygffcf2JQf5RnFi622klzb8JVxQkQ/thumb.jpg?format=300w" style="font-size: 0px; left: 0px; top: 0px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1557218838661-KUUDGYT8XLPXAJ70MZWZ/ke17ZwdGBToddI8pDm48kKY-G6HndK7v2FlTyTIJQI57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UWBFTp-jyz-u-yJnrxahDVdFAjR2sOZTixm23ygffcf2JQf5RnFi622klzb8JVxQkQ/thumb.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Hand Activities (2019)</div>
          </div>
        </a>

        <a class="project " href="/beamband/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020686362-TC2O63PYYS1E1HV0ZMW8/ke17ZwdGBToddI8pDm48kBVDUY_ojHUJPbTAKvjNhBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmuRu0Pfuf5F0yH2Y0n9Wb2uSXfk79yzklavJJ48t78dp27rAT2psOBuL_FmvkUhyS/beamband-thumb.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020686362-TC2O63PYYS1E1HV0ZMW8/ke17ZwdGBToddI8pDm48kBVDUY_ojHUJPbTAKvjNhBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmuRu0Pfuf5F0yH2Y0n9Wb2uSXfk79yzklavJJ48t78dp27rAT2psOBuL_FmvkUhyS/beamband-thumb.png" data-image-dimensions="1440x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="BeamBand: Hand Gesture Sensing with Ultrasonic Beamforming" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020686362-TC2O63PYYS1E1HV0ZMW8/ke17ZwdGBToddI8pDm48kBVDUY_ojHUJPbTAKvjNhBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmuRu0Pfuf5F0yH2Y0n9Wb2uSXfk79yzklavJJ48t78dp27rAT2psOBuL_FmvkUhyS/beamband-thumb.png?format=300w" style="font-size: 0px; left: -31px; top: 0px; width: 248px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020686362-TC2O63PYYS1E1HV0ZMW8/ke17ZwdGBToddI8pDm48kBVDUY_ojHUJPbTAKvjNhBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmuRu0Pfuf5F0yH2Y0n9Wb2uSXfk79yzklavJJ48t78dp27rAT2psOBuL_FmvkUhyS/beamband-thumb.png?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">BeamBand (2019)</div>
          </div>
        </a>

        <a class="project " href="/interferi/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020497815-WC0ADXGYWFIQ3OV78W9B/ke17ZwdGBToddI8pDm48kAegX-1irUL6qWVp5YHdPlZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0mjKWc80r5PD-660Rc3KKl-EIGwdh7NUl8wqqSWoiGqWWkt1uV2bgdYZttL59vHUoQ/interferi-thumb.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020497815-WC0ADXGYWFIQ3OV78W9B/ke17ZwdGBToddI8pDm48kAegX-1irUL6qWVp5YHdPlZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0mjKWc80r5PD-660Rc3KKl-EIGwdh7NUl8wqqSWoiGqWWkt1uV2bgdYZttL59vHUoQ/interferi-thumb.png" data-image-dimensions="2500x1404" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="Interferi: Gesture Sensing using On-Body Acoustic Interferometry" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020497815-WC0ADXGYWFIQ3OV78W9B/ke17ZwdGBToddI8pDm48kAegX-1irUL6qWVp5YHdPlZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0mjKWc80r5PD-660Rc3KKl-EIGwdh7NUl8wqqSWoiGqWWkt1uV2bgdYZttL59vHUoQ/interferi-thumb.png?format=500w" style="font-size: 0px; left: -72.5983px; top: 0px; width: 331.197px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1560020497815-WC0ADXGYWFIQ3OV78W9B/ke17ZwdGBToddI8pDm48kAegX-1irUL6qWVp5YHdPlZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0mjKWc80r5PD-660Rc3KKl-EIGwdh7NUl8wqqSWoiGqWWkt1uV2bgdYZttL59vHUoQ/interferi-thumb.png?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">Interferi (2019)</div>
          </div>
        </a>

        <a class="project " href="/eyespyvr-2018/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1582626995428-5VKLJEO84V51W4K5FVRZ/ke17ZwdGBToddI8pDm48kNXCUrrKjetGVMFIkBcnqREUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKc3XMixWOGMhAQ0lbIR9TAN-1tksE3g3DAinfzJCNIRh6Vo73cC2iPYZg8gDdHNqVP/eyespyvr.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1582626995428-5VKLJEO84V51W4K5FVRZ/ke17ZwdGBToddI8pDm48kNXCUrrKjetGVMFIkBcnqREUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKc3XMixWOGMhAQ0lbIR9TAN-1tksE3g3DAinfzJCNIRh6Vo73cC2iPYZg8gDdHNqVP/eyespyvr.png" data-image-dimensions="1202x676" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="eyespyvr.png" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1582626995428-5VKLJEO84V51W4K5FVRZ/ke17ZwdGBToddI8pDm48kNXCUrrKjetGVMFIkBcnqREUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKc3XMixWOGMhAQ0lbIR9TAN-1tksE3g3DAinfzJCNIRh6Vo73cC2iPYZg8gDdHNqVP/eyespyvr.png?format=500w" style="font-size: 0px; left: -72.3639px; top: 0px; width: 330.728px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1582626995428-5VKLJEO84V51W4K5FVRZ/ke17ZwdGBToddI8pDm48kNXCUrrKjetGVMFIkBcnqREUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKc3XMixWOGMhAQ0lbIR9TAN-1tksE3g3DAinfzJCNIRh6Vo73cC2iPYZg8gDdHNqVP/eyespyvr.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">EyeSpyVR (2018)</div>
          </div>
        </a>

        <a class="project " href="/ubicoustics/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935964979-SBIVY7SPTVAQPPX7I3VU/ke17ZwdGBToddI8pDm48kJK4Mm1kch8SFO9ZNkN1NT97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmFk_H6M1tkD9NpL7mXac0oVSXdFfjxR5AjcLwGSebOiGBsFzzcw3xKxvyC_6CFFG_/ubicoustics_figlab_thumb_square.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935964979-SBIVY7SPTVAQPPX7I3VU/ke17ZwdGBToddI8pDm48kJK4Mm1kch8SFO9ZNkN1NT97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmFk_H6M1tkD9NpL7mXac0oVSXdFfjxR5AjcLwGSebOiGBsFzzcw3xKxvyC_6CFFG_/ubicoustics_figlab_thumb_square.jpg" data-image-dimensions="1080x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="ubicoustics_figlab_thumb_square.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935964979-SBIVY7SPTVAQPPX7I3VU/ke17ZwdGBToddI8pDm48kJK4Mm1kch8SFO9ZNkN1NT97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmFk_H6M1tkD9NpL7mXac0oVSXdFfjxR5AjcLwGSebOiGBsFzzcw3xKxvyC_6CFFG_/ubicoustics_figlab_thumb_square.jpg?format=300w" style="font-size: 0px; left: 0px; top: 0px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935964979-SBIVY7SPTVAQPPX7I3VU/ke17ZwdGBToddI8pDm48kJK4Mm1kch8SFO9ZNkN1NT97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmFk_H6M1tkD9NpL7mXac0oVSXdFfjxR5AjcLwGSebOiGBsFzzcw3xKxvyC_6CFFG_/ubicoustics_figlab_thumb_square.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Ubicoustics (2018)</div>
          </div>
        </a>

        <a class="project " href="/vibrosight-2018/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935303388-PTKK568QOMZDZOZMWPTE/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/virbosight_thumb.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935303388-PTKK568QOMZDZOZMWPTE/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/virbosight_thumb.png" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="virbosight_thumb.png" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935303388-PTKK568QOMZDZOZMWPTE/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/virbosight_thumb.png?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1540935303388-PTKK568QOMZDZOZMWPTE/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UbeDbaZv1s3QfpIA4TYnL5Qao8BosUKjCVjCf8TKewJIH3bqxw7fF48mhrq5Ulr0Hg/virbosight_thumb.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Vibrosight (2018)</div>
          </div>
        </a>

        <a class="project " href="/wall-2018/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1525027170791-RFUBO18E1S74WKVT6PNT/ke17ZwdGBToddI8pDm48kF5UcQgsrEa5M_kXB3FEJvp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmT2Rk9u7KA3hFIM978bk874SrKaiC5ZucjOJLobqlQ4eSVvr4qR_0WyoaqS9aj771/wallImage.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1525027170791-RFUBO18E1S74WKVT6PNT/ke17ZwdGBToddI8pDm48kF5UcQgsrEa5M_kXB3FEJvp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmT2Rk9u7KA3hFIM978bk874SrKaiC5ZucjOJLobqlQ4eSVvr4qR_0WyoaqS9aj771/wallImage.jpg" data-image-dimensions="1081x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="wallImage.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1525027170791-RFUBO18E1S74WKVT6PNT/ke17ZwdGBToddI8pDm48kF5UcQgsrEa5M_kXB3FEJvp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmT2Rk9u7KA3hFIM978bk874SrKaiC5ZucjOJLobqlQ4eSVvr4qR_0WyoaqS9aj771/wallImage.jpg?format=300w" style="font-size: 0px; left: -0.0861111px; top: 0px; width: 186.172px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1525027170791-RFUBO18E1S74WKVT6PNT/ke17ZwdGBToddI8pDm48kF5UcQgsrEa5M_kXB3FEJvp7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmT2Rk9u7KA3hFIM978bk874SrKaiC5ZucjOJLobqlQ4eSVvr4qR_0WyoaqS9aj771/wallImage.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Wall++ (2018)</div>
          </div>
        </a>

        <a class="project " href="/pulp-nonfiction-2018/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524952934513-664HN1A4QL98K5P53AE2/ke17ZwdGBToddI8pDm48kE2nkgyDA1UzntOjn8KiXP8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcZWqU7au7RpBH6bEBlAuq1UAPgku1qy4EbJV8gGnYmGuc0H453KTcsFUHLgeTILFM/thumbnail.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524952934513-664HN1A4QL98K5P53AE2/ke17ZwdGBToddI8pDm48kE2nkgyDA1UzntOjn8KiXP8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcZWqU7au7RpBH6bEBlAuq1UAPgku1qy4EbJV8gGnYmGuc0H453KTcsFUHLgeTILFM/thumbnail.jpg" data-image-dimensions="1000x723" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="thumbnail.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524952934513-664HN1A4QL98K5P53AE2/ke17ZwdGBToddI8pDm48kE2nkgyDA1UzntOjn8KiXP8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcZWqU7au7RpBH6bEBlAuq1UAPgku1qy4EbJV8gGnYmGuc0H453KTcsFUHLgeTILFM/thumbnail.jpg?format=300w" style="font-size: 0px; left: -35.6307px; top: 0px; width: 257.261px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524952934513-664HN1A4QL98K5P53AE2/ke17ZwdGBToddI8pDm48kE2nkgyDA1UzntOjn8KiXP8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcZWqU7au7RpBH6bEBlAuq1UAPgku1qy4EbJV8gGnYmGuc0H453KTcsFUHLgeTILFM/thumbnail.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Pulp Nonfiction (2018)</div>
          </div>
        </a>

        <a class="project " href="/lumiwatch-2018/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524953931853-YBKDD0BCXSGQLZYM6WJ4/ke17ZwdGBToddI8pDm48kK5jQIYJ7N4srE-Px2oBNy17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UdiriEFMO0chXLLKbsU1cWUmjlWwd1qZR3HcYBGaMoPIZ5819XDE-T-fE_EmFUjQwQ/IMG_9503.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524953931853-YBKDD0BCXSGQLZYM6WJ4/ke17ZwdGBToddI8pDm48kK5jQIYJ7N4srE-Px2oBNy17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UdiriEFMO0chXLLKbsU1cWUmjlWwd1qZR3HcYBGaMoPIZ5819XDE-T-fE_EmFUjQwQ/IMG_9503.jpg" data-image-dimensions="2000x1343" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="IMG_9503.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524953931853-YBKDD0BCXSGQLZYM6WJ4/ke17ZwdGBToddI8pDm48kK5jQIYJ7N4srE-Px2oBNy17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UdiriEFMO0chXLLKbsU1cWUmjlWwd1qZR3HcYBGaMoPIZ5819XDE-T-fE_EmFUjQwQ/IMG_9503.jpg?format=300w" style="font-size: 0px; left: -45.4959px; top: 0px; width: 276.992px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1524953931853-YBKDD0BCXSGQLZYM6WJ4/ke17ZwdGBToddI8pDm48kK5jQIYJ7N4srE-Px2oBNy17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UdiriEFMO0chXLLKbsU1cWUmjlWwd1qZR3HcYBGaMoPIZ5819XDE-T-fE_EmFUjQwQ/IMG_9503.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">LumiWatch (2018)</div>
          </div>
        </a>

        <a class="project " href="/desktopography/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1511968018892-08UBFP2YDAC26Q7XE303/ke17ZwdGBToddI8pDm48kLtN5e6yN0zv2sXJfg70D24UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dqdBgw1X-01XsuwO7md6v74ORqr-BhWOGF_r6LtNcKfECjLISwBs8eEdxAxTptZAUg/Screen+Shot+2017-11-29+at+10.02.39+AM.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1511968018892-08UBFP2YDAC26Q7XE303/ke17ZwdGBToddI8pDm48kLtN5e6yN0zv2sXJfg70D24UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dqdBgw1X-01XsuwO7md6v74ORqr-BhWOGF_r6LtNcKfECjLISwBs8eEdxAxTptZAUg/Screen+Shot+2017-11-29+at+10.02.39+AM.png" data-image-dimensions="1761x805" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="Desktopography: Supporting Responsive Cohabitation Between Virtual Interfaces and Physical Objects" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1511968018892-08UBFP2YDAC26Q7XE303/ke17ZwdGBToddI8pDm48kLtN5e6yN0zv2sXJfg70D24UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dqdBgw1X-01XsuwO7md6v74ORqr-BhWOGF_r6LtNcKfECjLISwBs8eEdxAxTptZAUg/Screen+Shot+2017-11-29+at+10.02.39+AM.png?format=500w" style="font-size: 0px; left: -110.445px; top: 0px; width: 406.889px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1511968018892-08UBFP2YDAC26Q7XE303/ke17ZwdGBToddI8pDm48kLtN5e6yN0zv2sXJfg70D24UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dqdBgw1X-01XsuwO7md6v74ORqr-BhWOGF_r6LtNcKfECjLISwBs8eEdxAxTptZAUg/Screen+Shot+2017-11-29+at+10.02.39+AM.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Desktopography (2017)</div>
          </div>
        </a>

        <a class="project " href="/synthetic-sensors-2017/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967957339-F6WGHX3987EVOGLC4ZQJ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/00_a_hardware_closeup.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967957339-F6WGHX3987EVOGLC4ZQJ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/00_a_hardware_closeup.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Laput, G., Zhang, Y. and Harrison, C. 2017. Synthetic Sensors: Towards General-Purpose Sensing. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 3986-3999. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967957339-F6WGHX3987EVOGLC4ZQJ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/00_a_hardware_closeup.jpg?format=300w" style="font-size: 0px; left: -46.4721px; top: 0px; width: 278.944px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967957339-F6WGHX3987EVOGLC4ZQJ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/00_a_hardware_closeup.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Synthetic Sensors (2017)</div>
          </div>
        </a>

        <a class="project " href="/electrick-2017/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1495033752366-8EK0WHCGWCL42FDCKX6Z/ke17ZwdGBToddI8pDm48kOxLRGAfd8DHyn-VO-OS79h7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0usXCOu1ni7cqT7DpdWG1QXsxC4JUKCTfgXShWWMRL-jMKpWpWZXzvRmlf_9uuK6qg/electrick2.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1495033752366-8EK0WHCGWCL42FDCKX6Z/ke17ZwdGBToddI8pDm48kOxLRGAfd8DHyn-VO-OS79h7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0usXCOu1ni7cqT7DpdWG1QXsxC4JUKCTfgXShWWMRL-jMKpWpWZXzvRmlf_9uuK6qg/electrick2.png" data-image-dimensions="2500x1400" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Zhang, Y., Laput, G. and Harrison, C. 2017. Electrick: Low-Cost Touch Sensing Using Electric Field Tomography. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 1-14. " class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1495033752366-8EK0WHCGWCL42FDCKX6Z/ke17ZwdGBToddI8pDm48kOxLRGAfd8DHyn-VO-OS79h7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0usXCOu1ni7cqT7DpdWG1QXsxC4JUKCTfgXShWWMRL-jMKpWpWZXzvRmlf_9uuK6qg/electrick2.png?format=500w" style="font-size: 0px; left: -73.0714px; top: -1.42109e-14px; width: 332.143px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1495033752366-8EK0WHCGWCL42FDCKX6Z/ke17ZwdGBToddI8pDm48kOxLRGAfd8DHyn-VO-OS79h7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0usXCOu1ni7cqT7DpdWG1QXsxC4JUKCTfgXShWWMRL-jMKpWpWZXzvRmlf_9uuK6qg/electrick2.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Electrick (2017)</div>
          </div>
        </a>

        <a class="project " href="/deus-em-machina-2017/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967499324-T8HCVB9Y0RPLK106QUIV/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/lightApp+copy.JPG" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967499324-T8HCVB9Y0RPLK106QUIV/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/lightApp+copy.JPG" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Xiao, R., Laput, G., Zhang, Y. and Harrison, C. 2017. Deus EM Machina: On-Touch Contextual Functionality for Smart IoT Appliances. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 4000-4008. " class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967499324-T8HCVB9Y0RPLK106QUIV/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/lightApp+copy.JPG?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967499324-T8HCVB9Y0RPLK106QUIV/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/lightApp+copy.JPG?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Deus EM Machina (2017)</div>
          </div>
        </a>

        <a class="project " href="/direct-2016/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967279868-EKKM2E0MMBH59YKK8697/ke17ZwdGBToddI8pDm48kIzMzxw83ArgtpSeIUQtitR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UR4XLi_vaDzitZ4haSey1nHVmem7cBq312l8OP4z1pGq0P5U1bFzSEwMxHQE91vu_A/compare.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967279868-EKKM2E0MMBH59YKK8697/ke17ZwdGBToddI8pDm48kIzMzxw83ArgtpSeIUQtitR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UR4XLi_vaDzitZ4haSey1nHVmem7cBq312l8OP4z1pGq0P5U1bFzSEwMxHQE91vu_A/compare.jpg" data-image-dimensions="2000x2050" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="compare.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967279868-EKKM2E0MMBH59YKK8697/ke17ZwdGBToddI8pDm48kIzMzxw83ArgtpSeIUQtitR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UR4XLi_vaDzitZ4haSey1nHVmem7cBq312l8OP4z1pGq0P5U1bFzSEwMxHQE91vu_A/compare.jpg?format=300w" style="font-size: 0px; left: 0px; top: -2.325px; width: 186px; height: 190.65px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967279868-EKKM2E0MMBH59YKK8697/ke17ZwdGBToddI8pDm48kIzMzxw83ArgtpSeIUQtitR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UR4XLi_vaDzitZ4haSey1nHVmem7cBq312l8OP4z1pGq0P5U1bFzSEwMxHQE91vu_A/compare.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">DIRECT (2016)</div>
          </div>
        </a>

        <a class="project " href="/capcam-2016/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967015114-MNW3LQBJU63LQHNQCEYU/ke17ZwdGBToddI8pDm48kPw0bXfz3hjgnTAlZZLoJIl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UegSNDBKjtoREifFChppomv_7sZ1DyunrJoaxYrOLmJqGG0XwADNPuScWbN_Q97KlA/kbd.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967015114-MNW3LQBJU63LQHNQCEYU/ke17ZwdGBToddI8pDm48kPw0bXfz3hjgnTAlZZLoJIl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UegSNDBKjtoREifFChppomv_7sZ1DyunrJoaxYrOLmJqGG0XwADNPuScWbN_Q97KlA/kbd.jpg" data-image-dimensions="2371x1200" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="kbd.jpg" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967015114-MNW3LQBJU63LQHNQCEYU/ke17ZwdGBToddI8pDm48kPw0bXfz3hjgnTAlZZLoJIl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UegSNDBKjtoREifFChppomv_7sZ1DyunrJoaxYrOLmJqGG0XwADNPuScWbN_Q97KlA/kbd.jpg?format=500w" style="font-size: 0px; left: -90.7525px; top: 0px; width: 367.505px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1494967015114-MNW3LQBJU63LQHNQCEYU/ke17ZwdGBToddI8pDm48kPw0bXfz3hjgnTAlZZLoJIl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UegSNDBKjtoREifFChppomv_7sZ1DyunrJoaxYrOLmJqGG0XwADNPuScWbN_Q97KlA/kbd.jpg?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">CapCam (2016)</div>
          </div>
        </a>

        <a class="project " href="/viband-2016/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535819652-QXFZG7GPL27H3TCMQ6Y9/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/VideoThumbnail.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535819652-QXFZG7GPL27H3TCMQ6Y9/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/VideoThumbnail.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="VideoThumbnail.jpg" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535819652-QXFZG7GPL27H3TCMQ6Y9/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/VideoThumbnail.jpg?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535819652-QXFZG7GPL27H3TCMQ6Y9/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/VideoThumbnail.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">ViBand (2016)</div>
          </div>
        </a>

        <a class="project " href="/aurasense-2016/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478534959675-HC4GD10Y8QJ0EAKIO5KK/ke17ZwdGBToddI8pDm48kEDgYR9XbNp02iaWmThWwtpZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwmOgeyy797S7k3Uv0CFPgtkH9JB6gMToD1APwWP_R3ECMa-tkANRNErJV8Pek5-Ak/thumbnail.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478534959675-HC4GD10Y8QJ0EAKIO5KK/ke17ZwdGBToddI8pDm48kEDgYR9XbNp02iaWmThWwtpZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwmOgeyy797S7k3Uv0CFPgtkH9JB6gMToD1APwWP_R3ECMa-tkANRNErJV8Pek5-Ak/thumbnail.jpg" data-image-dimensions="668x676" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="thumbnail.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478534959675-HC4GD10Y8QJ0EAKIO5KK/ke17ZwdGBToddI8pDm48kEDgYR9XbNp02iaWmThWwtpZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwmOgeyy797S7k3Uv0CFPgtkH9JB6gMToD1APwWP_R3ECMa-tkANRNErJV8Pek5-Ak/thumbnail.jpg?format=300w" style="font-size: 0px; left: 0px; top: -1.11377px; width: 186px; height: 188.228px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478534959675-HC4GD10Y8QJ0EAKIO5KK/ke17ZwdGBToddI8pDm48kEDgYR9XbNp02iaWmThWwtpZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwmOgeyy797S7k3Uv0CFPgtkH9JB6gMToD1APwWP_R3ECMa-tkANRNErJV8Pek5-Ak/thumbnail.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">AuraSense (2016)</div>
          </div>
        </a>

        <a class="project " href="/tomo2/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535923822-G8WVGJXCB2C0KCQK45LC/ke17ZwdGBToddI8pDm48kH57sgD1tvYEeMmeWW5KQ1JZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVGP3PEk-4tU-lRdiu5v5GR_LnhVR9M5gwz1lz67kkv7Mxur-lC0WofN0YB1wFg-ZW0/Tomo2.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535923822-G8WVGJXCB2C0KCQK45LC/ke17ZwdGBToddI8pDm48kH57sgD1tvYEeMmeWW5KQ1JZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVGP3PEk-4tU-lRdiu5v5GR_LnhVR9M5gwz1lz67kkv7Mxur-lC0WofN0YB1wFg-ZW0/Tomo2.png" data-image-dimensions="398x398" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="Tomo2.png" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535923822-G8WVGJXCB2C0KCQK45LC/ke17ZwdGBToddI8pDm48kH57sgD1tvYEeMmeWW5KQ1JZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVGP3PEk-4tU-lRdiu5v5GR_LnhVR9M5gwz1lz67kkv7Mxur-lC0WofN0YB1wFg-ZW0/Tomo2.png?format=300w" style="font-size: 0px; left: 0px; top: 0px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1478535923822-G8WVGJXCB2C0KCQK45LC/ke17ZwdGBToddI8pDm48kH57sgD1tvYEeMmeWW5KQ1JZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVGP3PEk-4tU-lRdiu5v5GR_LnhVR9M5gwz1lz67kkv7Mxur-lC0WofN0YB1wFg-ZW0/Tomo2.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Tomo 2 (2016)</div>
          </div>
        </a>

        <a class="project " href="/skintrack-2016/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1463781344024-KZ4EGSHQQTP71W1OGZF3/ke17ZwdGBToddI8pDm48kIwIYiWyC_BowNPKWsMP7zd7gQa3H78H3Y0txjaiv_0fA_azzaaRoizB4zTDpO0WY4C9CpLsLRLX79j00ozwXJvyv50FWYIo8g54Jb1bvKqqZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWmv6jgPLG1XMKvWKgpsAEmcUxbqppkj50EWvjTwI_JTQ/k.JPG" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1463781344024-KZ4EGSHQQTP71W1OGZF3/ke17ZwdGBToddI8pDm48kIwIYiWyC_BowNPKWsMP7zd7gQa3H78H3Y0txjaiv_0fA_azzaaRoizB4zTDpO0WY4C9CpLsLRLX79j00ozwXJvyv50FWYIo8g54Jb1bvKqqZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWmv6jgPLG1XMKvWKgpsAEmcUxbqppkj50EWvjTwI_JTQ/k.JPG" data-image-dimensions="5184x2912" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="k.JPG" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1463781344024-KZ4EGSHQQTP71W1OGZF3/ke17ZwdGBToddI8pDm48kIwIYiWyC_BowNPKWsMP7zd7gQa3H78H3Y0txjaiv_0fA_azzaaRoizB4zTDpO0WY4C9CpLsLRLX79j00ozwXJvyv50FWYIo8g54Jb1bvKqqZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWmv6jgPLG1XMKvWKgpsAEmcUxbqppkj50EWvjTwI_JTQ/k.JPG?format=500w" style="font-size: 0px; left: -72.5604px; top: 0px; width: 331.121px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1463781344024-KZ4EGSHQQTP71W1OGZF3/ke17ZwdGBToddI8pDm48kIwIYiWyC_BowNPKWsMP7zd7gQa3H78H3Y0txjaiv_0fA_azzaaRoizB4zTDpO0WY4C9CpLsLRLX79j00ozwXJvyv50FWYIo8g54Jb1bvKqqZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWmv6jgPLG1XMKvWKgpsAEmcUxbqppkj50EWvjTwI_JTQ/k.JPG?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">SkinTrack (2016)</div>
          </div>
        </a>

        <a class="project " href="/sweepsense/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1462496084073-4DJBG92U1CKT3THNFS3B/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fgkxBWjjRb1N_UxbACPPq6o8P4ND6591eyixB1seuwXZBP4j2ZMngv5acyYAR_NT0OqpeNLcJ80NK65_fV7S1Ue_lng9z3wQYMvDa7lbYHhnz0No4EgkJWSWh15s-BrG0_CIL1ZYFuTARAFyC6T_zvA/02_Right_Out.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1462496084073-4DJBG92U1CKT3THNFS3B/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fgkxBWjjRb1N_UxbACPPq6o8P4ND6591eyixB1seuwXZBP4j2ZMngv5acyYAR_NT0OqpeNLcJ80NK65_fV7S1Ue_lng9z3wQYMvDa7lbYHhnz0No4EgkJWSWh15s-BrG0_CIL1ZYFuTARAFyC6T_zvA/02_Right_Out.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-parent-ratio="1.0" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1462496084073-4DJBG92U1CKT3THNFS3B/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fgkxBWjjRb1N_UxbACPPq6o8P4ND6591eyixB1seuwXZBP4j2ZMngv5acyYAR_NT0OqpeNLcJ80NK65_fV7S1Ue_lng9z3wQYMvDa7lbYHhnz0No4EgkJWSWh15s-BrG0_CIL1ZYFuTARAFyC6T_zvA/02_Right_Out.jpg?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1462496084073-4DJBG92U1CKT3THNFS3B/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fgkxBWjjRb1N_UxbACPPq6o8P4ND6591eyixB1seuwXZBP4j2ZMngv5acyYAR_NT0OqpeNLcJ80NK65_fV7S1Ue_lng9z3wQYMvDa7lbYHhnz0No4EgkJWSWh15s-BrG0_CIL1ZYFuTARAFyC6T_zvA/02_Right_Out.jpg?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">SweepSense (2016)</div>
          </div>
        </a>

        <a class="project " href="/fingerpose/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895629619-YX0OWEMUW6UEKV98M64P/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895629619-YX0OWEMUW6UEKV98M64P/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Xiao, R. Schwarz, J. and Harrison, C. Estimating 3D Finger Angle on Commodity Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ‘15). ACM, New York, NY. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895629619-YX0OWEMUW6UEKV98M64P/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895629619-YX0OWEMUW6UEKV98M64P/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">FingerPose (2015)</div>
          </div>
        </a>

        <a class="project " href="/capauth/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895951793-Y8KAL1K9NPV7OVS5YILH/ke17ZwdGBToddI8pDm48kJaY95dotR99Y3f9Nm8ToPF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCYWGxfdB_uf1_ERfebHZ44G_2GqWQDW_nyQTS8zPi5D2Ogm_Ha3R8LNAReSVjEjA/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895951793-Y8KAL1K9NPV7OVS5YILH/ke17ZwdGBToddI8pDm48kJaY95dotR99Y3f9Nm8ToPF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCYWGxfdB_uf1_ERfebHZ44G_2GqWQDW_nyQTS8zPi5D2Ogm_Ha3R8LNAReSVjEjA/image-asset.jpeg" data-image-dimensions="3957x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Guo, A., Xiao, R. and Harrison, C. 2015. CapAuth: Identifying and Differentiating User Handprints on Commodity Capacitive Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (Madeira, Portugal, November 15 - 18, 2015). ITS ‘15. ACM, New York, NY. 59-62. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895951793-Y8KAL1K9NPV7OVS5YILH/ke17ZwdGBToddI8pDm48kJaY95dotR99Y3f9Nm8ToPF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCYWGxfdB_uf1_ERfebHZ44G_2GqWQDW_nyQTS8zPi5D2Ogm_Ha3R8LNAReSVjEjA/image-asset.jpeg?format=300w" style="font-size: 0px; left: -13.4818px; top: 0px; width: 212.964px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450895951793-Y8KAL1K9NPV7OVS5YILH/ke17ZwdGBToddI8pDm48kJaY95dotR99Y3f9Nm8ToPF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCYWGxfdB_uf1_ERfebHZ44G_2GqWQDW_nyQTS8zPi5D2Ogm_Ha3R8LNAReSVjEjA/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">CapAuth (2015)</div>
          </div>
        </a>

        <a class="project " href="/gazegesture/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896165931-F4O6HUDKKIBCTO0O30SB/ke17ZwdGBToddI8pDm48kB6N0s8PWtX2k_eW8krg04V7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1URWK2DJDpV27WG7FD5VZsfFVodF6E_6KI51EW1dNf095hdyjf10zfCEVHp52s13p8g/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896165931-F4O6HUDKKIBCTO0O30SB/ke17ZwdGBToddI8pDm48kB6N0s8PWtX2k_eW8krg04V7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1URWK2DJDpV27WG7FD5VZsfFVodF6E_6KI51EW1dNf095hdyjf10zfCEVHp52s13p8g/image-asset.jpeg" data-image-dimensions="1800x1200" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Chatterjee, I., Xiao, R. and Harrison, C. 2015. Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions. In Proceedings of the 17th ACM International Conference on Multimodal Interaction (Seattle, Washington, November 9 - 13, 2015). ICMI '15. ACM, New York, NY. 131-138. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896165931-F4O6HUDKKIBCTO0O30SB/ke17ZwdGBToddI8pDm48kB6N0s8PWtX2k_eW8krg04V7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1URWK2DJDpV27WG7FD5VZsfFVodF6E_6KI51EW1dNf095hdyjf10zfCEVHp52s13p8g/image-asset.jpeg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896165931-F4O6HUDKKIBCTO0O30SB/ke17ZwdGBToddI8pDm48kB6N0s8PWtX2k_eW8krg04V7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1URWK2DJDpV27WG7FD5VZsfFVodF6E_6KI51EW1dNf095hdyjf10zfCEVHp52s13p8g/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Gaze+Gesture (2015)</div>
          </div>
        </a>

        <a class="project " href="/emsense/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896338545-QUXSR8S65XNBICRAUBZP/ke17ZwdGBToddI8pDm48kGwvvILFRU8TVLtQPFuX3HB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UYdWN69ZQzgEqKIIOw4FvIvDIvENyykEVQO8nSDRMvqgBBBFbt9YekZp4EE0dHePbg/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896338545-QUXSR8S65XNBICRAUBZP/ke17ZwdGBToddI8pDm48kGwvvILFRU8TVLtQPFuX3HB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UYdWN69ZQzgEqKIIOw4FvIvDIvENyykEVQO8nSDRMvqgBBBFbt9YekZp4EE0dHePbg/image-asset.jpeg" data-image-dimensions="2136x1536" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Laput, G., Yang, C. Xiao, R, Sample, A. and Harrison, C. 2015. EM-Sense: Touch Recognition of Uninstrumented, Electrical and Electromechanical Objects. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (Charlotte, North Carolina, November 8 - 11, 2015). UIST '15. ACM, New York, NY. 157-166. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896338545-QUXSR8S65XNBICRAUBZP/ke17ZwdGBToddI8pDm48kGwvvILFRU8TVLtQPFuX3HB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UYdWN69ZQzgEqKIIOw4FvIvDIvENyykEVQO8nSDRMvqgBBBFbt9YekZp4EE0dHePbg/image-asset.jpeg?format=300w" style="font-size: 0px; left: -36.3281px; top: 0px; width: 258.656px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896338545-QUXSR8S65XNBICRAUBZP/ke17ZwdGBToddI8pDm48kGwvvILFRU8TVLtQPFuX3HB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UYdWN69ZQzgEqKIIOw4FvIvDIvENyykEVQO8nSDRMvqgBBBFbt9YekZp4EE0dHePbg/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">EM-Sense (2015)</div>
          </div>
        </a>

        <a class="project " href="/tomo/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896794079-659WG0FUK6UWHYCRYPTM/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_0259.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896794079-659WG0FUK6UWHYCRYPTM/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_0259.jpg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="IMG_0259.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896794079-659WG0FUK6UWHYCRYPTM/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_0259.jpg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450896794079-659WG0FUK6UWHYCRYPTM/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_0259.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Tomo (2015)</div>
          </div>
        </a>

        <a class="project " href="/furbrication/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897063144-SJSRRE0KM6DTJZ086RP1/ke17ZwdGBToddI8pDm48kFks_xkDqLFJN7QRPNFThvZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIAX39JxkiRiz2jnTDbU4pwdgFcU9n8vfeXFSeckIS65kKMshLAGzx4R3EDFOm1kBS/horse.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897063144-SJSRRE0KM6DTJZ086RP1/ke17ZwdGBToddI8pDm48kFks_xkDqLFJN7QRPNFThvZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIAX39JxkiRiz2jnTDbU4pwdgFcU9n8vfeXFSeckIS65kKMshLAGzx4R3EDFOm1kBS/horse.jpg" data-image-dimensions="920x614" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="horse.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897063144-SJSRRE0KM6DTJZ086RP1/ke17ZwdGBToddI8pDm48kFks_xkDqLFJN7QRPNFThvZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIAX39JxkiRiz2jnTDbU4pwdgFcU9n8vfeXFSeckIS65kKMshLAGzx4R3EDFOm1kBS/horse.jpg?format=300w" style="font-size: 0px; left: -46.3485px; top: -1.42109e-14px; width: 278.697px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897063144-SJSRRE0KM6DTJZ086RP1/ke17ZwdGBToddI8pDm48kFks_xkDqLFJN7QRPNFThvZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIAX39JxkiRiz2jnTDbU4pwdgFcU9n8vfeXFSeckIS65kKMshLAGzx4R3EDFOm1kBS/horse.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">3D-Printed Hair (2015)</div>
          </div>
        </a>

        <a class="project " href="/zensors/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897334179-LJQA2YENVP1CKTISHI7A/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/zensors3.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897334179-LJQA2YENVP1CKTISHI7A/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/zensors3.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="zensors3.jpg" class="" data-image-resolution="500w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897334179-LJQA2YENVP1CKTISHI7A/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/zensors3.jpg?format=500w" style="font-size: 0px; left: -72.3333px; top: 0px; width: 330.667px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897334179-LJQA2YENVP1CKTISHI7A/ke17ZwdGBToddI8pDm48kNvT88LknE-K9M4pGNO0Iqd7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USOFn4xF8vTWDNAUBm5ducQhX-V3oVjSmr829Rco4W2Uo49ZdOtO_QXox0_W7i2zEA/zensors3.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Zensors (2015)</div>
          </div>
        </a>

        <a class="project " href="/new-gallery/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897463888-TR9WC8MO62336H90GW4J/ke17ZwdGBToddI8pDm48kOggE0Ch6pMGalwtLMqzsSB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1Ufo5RWkg_J4of0jUNHaDHx6pZKBvpVYzidBWCapg0tuoMuEaB2HPGSYDV-11UTcW2g/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897463888-TR9WC8MO62336H90GW4J/ke17ZwdGBToddI8pDm48kOggE0Ch6pMGalwtLMqzsSB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1Ufo5RWkg_J4of0jUNHaDHx6pZKBvpVYzidBWCapg0tuoMuEaB2HPGSYDV-11UTcW2g/image-asset.jpeg" data-image-dimensions="1920x1280" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Laput, G., Brockmeyer, E., Hudson, S. and Harrison, C. 2015. Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices. In Proceedings of the 33nd Annual SIGCHI Conference on Human Factors in Computing Systems (Seoul, Korea, April 18 - 23, 2015). CHI '15. ACM, New York, NY. 2161-2170. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897463888-TR9WC8MO62336H90GW4J/ke17ZwdGBToddI8pDm48kOggE0Ch6pMGalwtLMqzsSB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1Ufo5RWkg_J4of0jUNHaDHx6pZKBvpVYzidBWCapg0tuoMuEaB2HPGSYDV-11UTcW2g/image-asset.jpeg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897463888-TR9WC8MO62336H90GW4J/ke17ZwdGBToddI8pDm48kOggE0Ch6pMGalwtLMqzsSB7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1Ufo5RWkg_J4of0jUNHaDHx6pZKBvpVYzidBWCapg0tuoMuEaB2HPGSYDV-11UTcW2g/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Acoustruments (2015)</div>
          </div>
        </a>

        <a class="project " href="/skinbuttons/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897780089-MS9T8XA80014SCZHQVE9/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897780089-MS9T8XA80014SCZHQVE9/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Laput, G., Xiao, R., Chen, X., Hudson, S. and Harrison, C. 2014. Skin Buttons: Cheap, Small, Low-Power and Clickable Fixed-Icon Laser Projections. In Proceedings of the 27th Annual ACM Symposium on User interface Software and Technology (Honolulu, Hawaii, October 5 - 8, 2014). UIST '14. ACM, New York, NY. 389-394. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897780089-MS9T8XA80014SCZHQVE9/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450897780089-MS9T8XA80014SCZHQVE9/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Skin Buttons (2014)</div>
          </div>
        </a>

        <a class="project " href="/airplustouch/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898176490-V565TRHYL3KQH3QOT4Q6/ke17ZwdGBToddI8pDm48kEDNU7O2UXYHHYHy4VvLHyZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0o8OMvY5tuV_wqZQCqqStn7bJnbI3Ui6ihneTf_Paoao2KFfGYQkqUnMWIzQQW5hyA/generic.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898176490-V565TRHYL3KQH3QOT4Q6/ke17ZwdGBToddI8pDm48kEDNU7O2UXYHHYHy4VvLHyZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0o8OMvY5tuV_wqZQCqqStn7bJnbI3Ui6ihneTf_Paoao2KFfGYQkqUnMWIzQQW5hyA/generic.jpg" data-image-dimensions="2575x1716" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="generic.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898176490-V565TRHYL3KQH3QOT4Q6/ke17ZwdGBToddI8pDm48kEDNU7O2UXYHHYHy4VvLHyZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0o8OMvY5tuV_wqZQCqqStn7bJnbI3Ui6ihneTf_Paoao2KFfGYQkqUnMWIzQQW5hyA/generic.jpg?format=300w" style="font-size: 0px; left: -46.5542px; top: 0px; width: 279.108px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898176490-V565TRHYL3KQH3QOT4Q6/ke17ZwdGBToddI8pDm48kEDNU7O2UXYHHYHy4VvLHyZ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0o8OMvY5tuV_wqZQCqqStn7bJnbI3Ui6ihneTf_Paoao2KFfGYQkqUnMWIzQQW5hyA/generic.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Air+Touch (2014)</div>
          </div>
        </a>

        <a class="project " href="/teslatouch/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898587483-BZX6IFMN3JCG53BCELSR/ke17ZwdGBToddI8pDm48kIyxqolMGeifrXk5l1Z2aZ17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCvyAd1-5UQFnp8aARaJsUTsNYOusfyas6PktyJBTwMQPIGC4nXg-teTjCMRgEFiQ/toffee.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898587483-BZX6IFMN3JCG53BCELSR/ke17ZwdGBToddI8pDm48kIyxqolMGeifrXk5l1Z2aZ17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCvyAd1-5UQFnp8aARaJsUTsNYOusfyas6PktyJBTwMQPIGC4nXg-teTjCMRgEFiQ/toffee.jpg" data-image-dimensions="3456x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="toffee.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898587483-BZX6IFMN3JCG53BCELSR/ke17ZwdGBToddI8pDm48kIyxqolMGeifrXk5l1Z2aZ17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCvyAd1-5UQFnp8aARaJsUTsNYOusfyas6PktyJBTwMQPIGC4nXg-teTjCMRgEFiQ/toffee.jpg?format=300w" style="font-size: 0px; left: 0px; top: 0px; width: 186px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450898587483-BZX6IFMN3JCG53BCELSR/ke17ZwdGBToddI8pDm48kIyxqolMGeifrXk5l1Z2aZ17gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lCvyAd1-5UQFnp8aARaJsUTsNYOusfyas6PktyJBTwMQPIGC4nXg-teTjCMRgEFiQ/toffee.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Toffee (2014)</div>
          </div>
        </a>

        <a class="project " href="/touchtools/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447789812527-ZD2ZQCJRA87VSS8BY5Q4/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447789812527-ZD2ZQCJRA87VSS8BY5Q4/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Harrison, C., Xiao, R., Schwarz, J., and Hudson, S. TouchTools: Leveraging Familiarity and Skill with Physical Tools to Augment Touch Interaction. In Proceedings of the 32nd Annual SIGCHI Conference on Human Factors in Computing Systems (Toronto, Canada, April 26 - May 1, 2014). CHI '14. ACM, New York, NY. 2913-2916. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447789812527-ZD2ZQCJRA87VSS8BY5Q4/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447789812527-ZD2ZQCJRA87VSS8BY5Q4/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">TouchTools (2014)</div>
          </div>
        </a>

        <a class="project " href="/smartwatch-5dof/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450899164072-VJUW5SNI42EMW31FS6RL/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_3566+X.jpg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450899164072-VJUW5SNI42EMW31FS6RL/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_3566+X.jpg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt="IMG_3566 X.jpg" class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450899164072-VJUW5SNI42EMW31FS6RL/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_3566+X.jpg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1450899164072-VJUW5SNI42EMW31FS6RL/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/IMG_3566+X.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Smartwatch 5DOF (2014)</div>
          </div>
        </a>

        <a class="project " href="/tapsense/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790172793-ESDG2ZRE268GNJ1RBK42/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790172793-ESDG2ZRE268GNJ1RBK42/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Harrison, C., Schwarz, J. and Hudson S. E. 2011. TapSense: Enhancing Finger Interaction on Touch Surfaces. In Proceedings of the 24th Annual ACM Symposium on User interface Software and Technology. UIST '11. ACM, New York, NY. 627-636. " class="" data-image-resolution="300w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790172793-ESDG2ZRE268GNJ1RBK42/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=300w" style="font-size: 0px; left: -46.5px; top: 0px; width: 279px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790172793-ESDG2ZRE268GNJ1RBK42/ke17ZwdGBToddI8pDm48kKV0nEQLrZCZyN20-sdpYlF7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0ivq7Q1ckvJa8MA8qNUlEOZ5IGU-1O_EUPktdRWJqBeswtSHa-_ZlYvzXGRIA25Fyg/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">TapSense (2011)</div>
          </div>
        </a>

        <a class="project " href="/teslatouch-1/">
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill" style="overflow: hidden;"><img data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790296744-4W7UH8NE598OATKOL7LZ/ke17ZwdGBToddI8pDm48kBfJC82zr283-cptPlVKgQIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcFt3EflJyNsqD_uNZ9Iq801j8fuIfQPk1oLr34Z1h6J52ksZGCORNbuHLRu5woUzZ/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790296744-4W7UH8NE598OATKOL7LZ/ke17ZwdGBToddI8pDm48kBfJC82zr283-cptPlVKgQIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcFt3EflJyNsqD_uNZ9Iq801j8fuIfQPk1oLr34Z1h6J52ksZGCORNbuHLRu5woUzZ/image-asset.jpeg" data-image-dimensions="1000x350" data-image-focal-point="0.5,0.5" data-load="false" data-parent-ratio="1.0" alt=" Bau, O., Poupyrev, I., Israr, A., and Harrison, C. 2010. TeslaTouch: Electrovibration for Touch Surfaces. In Proceedings of the 23rd Annual ACM Symposium on User interface Software and Technology (New York, New York, October 3 - 6, 2010). UIST '10. ACM, New York, NY. 283-292. " class="" data-image-resolution="750w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790296744-4W7UH8NE598OATKOL7LZ/ke17ZwdGBToddI8pDm48kBfJC82zr283-cptPlVKgQIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcFt3EflJyNsqD_uNZ9Iq801j8fuIfQPk1oLr34Z1h6J52ksZGCORNbuHLRu5woUzZ/image-asset.jpeg?format=750w" style="font-size: 0px; left: -172.714px; top: -1.42109e-14px; width: 531.429px; height: 186px; position: relative;"><noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1447790296744-4W7UH8NE598OATKOL7LZ/ke17ZwdGBToddI8pDm48kBfJC82zr283-cptPlVKgQIUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcFt3EflJyNsqD_uNZ9Iq801j8fuIfQPk1oLr34Z1h6J52ksZGCORNbuHLRu5woUzZ/image-asset.jpeg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">TeslaTouch (2010)</div>
          </div>
        </a>



  </div>
</div>
    </section>

    <div class="extra-wrapper page-footer" id="yui_3_17_2_1_1618546122048_92">
      <div class="sqs-layout sqs-grid-12 columns-12" data-layout-label="Footer Content: Research" data-type="block-field" data-updated-on="1515097656775" id="page-footer-564b5638e4b0cfc22513bc9c"><div class="row sqs-row" id="yui_3_17_2_1_1618546122048_91"><div class="col sqs-col-12 span-12" id="yui_3_17_2_1_1618546122048_90"><div class="sqs-block horizontalrule-block sqs-block-horizontalrule" data-block-type="47" id="block-yui_3_17_2_15_1447816420419_5558"><div class="sqs-block-content"><hr></div></div><div class="row sqs-row" id="yui_3_17_2_1_1618546122048_89"><div class="col sqs-col-2 span-2" id="yui_3_17_2_1_1618546122048_88"><div class="sqs-block image-block sqs-block-image sqs-text-ready" data-aspect-ratio="56.875" data-block-type="5" id="block-yui_3_17_2_24_1450453695280_16504"><div class="sqs-block-content" id="yui_3_17_2_1_1618546122048_87">












    <div class="image-block-outer-wrapper
          layout-caption-hidden
          design-layout-inline



         sqs-narrow-width" data-test="image-block-inline-outer-wrapper" id="yui_3_17_2_1_1618546122048_86">




        <figure class="
              sqs-block-image-figure
              intrinsic
            " style="max-width:1667px;" id="yui_3_17_2_1_1618546122048_85">





          <a class="
                sqs-block-image-link



              " href="http://cmu.edu" target="_blank" id="yui_3_17_2_1_1618546122048_84">

          <div style="padding-bottom: 56.875%; overflow: hidden;" class="
                image-block-wrapper



                has-aspect-ratio
              " data-animation-role="image" data-animation-override="" id="yui_3_17_2_1_1618546122048_83">
            <noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097210363-S72PU5DJQZDGC0XVJ3JP/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/cmu2.png" alt="cmu2.png" /></noscript><img class="thumb-image loaded" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097210363-S72PU5DJQZDGC0XVJ3JP/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/cmu2.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097210363-S72PU5DJQZDGC0XVJ3JP/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/cmu2.png" data-image-dimensions="1667x946" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a4e8c790d9297b7533f8cbd" data-type="image" alt="cmu2.png" data-image-resolution="100w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097210363-S72PU5DJQZDGC0XVJ3JP/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/cmu2.png?format=100w" style="left: 0%; top: -0.364427%; width: 100%; height: 100.729%; position: absolute;">
          </div>

          </a>




        </figure>


    </div>






</div></div></div><div class="col sqs-col-10 span-10" id="yui_3_17_2_1_1618546122048_112"><div class="sqs-block image-block sqs-block-image sqs-col-2 span-2 float float-right sqs-text-ready" data-aspect-ratio="56.875" data-block-type="5" id="block-yui_3_17_2_24_1450453695280_17453"><div class="sqs-block-content" id="yui_3_17_2_1_1618546122048_111">












    <div class="image-block-outer-wrapper
          layout-caption-hidden
          design-layout-inline



         sqs-narrow-width" data-test="image-block-inline-outer-wrapper" id="yui_3_17_2_1_1618546122048_110">




        <figure class="
              sqs-block-image-figure
              intrinsic
            " style="max-width:1667px;" id="yui_3_17_2_1_1618546122048_109">





          <a class="
                sqs-block-image-link



              " href="http://hcii.cmu.edu" target="_blank" id="yui_3_17_2_1_1618546122048_108">

          <div style="padding-bottom: 56.875%; overflow: hidden;" class="
                image-block-wrapper



                has-aspect-ratio
              " data-animation-role="image" data-animation-override="" id="yui_3_17_2_1_1618546122048_107">
            <noscript><img src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097232812-SJ2JFZS4L6SIUNHRYBYG/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/hcii2.png" alt="hcii2.png" /></noscript><img class="thumb-image loaded" data-src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097232812-SJ2JFZS4L6SIUNHRYBYG/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/hcii2.png" data-image="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097232812-SJ2JFZS4L6SIUNHRYBYG/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/hcii2.png" data-image-dimensions="1667x946" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a4e8c9071c10b1be8992c02" data-type="image" alt="hcii2.png" data-image-resolution="100w" src="https://images.squarespace-cdn.com/content/v1/564b5107e4b087849452ea4b/1515097232812-SJ2JFZS4L6SIUNHRYBYG/ke17ZwdGBToddI8pDm48kNAAkxUPS_167KSvLYPbwWAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dl-FXpJSj7WyeLRdbLIyeOZwOOtA51ZRMksmef40AshPCjLISwBs8eEdxAxTptZAUg/hcii2.png?format=100w" style="left: -0.347327%; top: 0%; width: 100.695%; height: 100%; position: absolute;">
          </div>

          </a>




        </figure>


    </div>






</div></div><div class="sqs-block html-block sqs-block-html" data-block-type="2" id="block-yui_3_17_2_24_1450453695280_16023"><div class="sqs-block-content"><p class="text-align-center">Future Interfaces Group<br><a target="_blank" href="http://hcii.cmu.edu">Human-Computer Interaction Institute</a><br><a href="http://cs.cmu.edu">School of Computer Science</a>&nbsp;<br><a href="http://cmu.edu">Carnegie Mellon University</a>&nbsp;</p></div></div></div></div></div></div></div>
    </div>

    <div class="page-divider"></div>

    <footer id="footer">
      <div class="sqs-layout sqs-grid-1 columns-1 empty" data-layout-label="Footer Content" data-type="block-field" data-updated-on="1450819064801" id="footerBlock"><div class="row sqs-row"><div class="col sqs-col-1 span-1"></div></div></div>



    </footer>

  </div>

  <div></div>

  <script type="text/javascript" src="https://static1.squarespace.com/static/ta/564b5107e4b087849452ea4b/0/scripts/combo/?dynamic-data.js&amp;site.js"></script>


  <script type="text/javascript" data-sqs-type="imageloader-bootstrapper">(function() {if(window.ImageLoader) { window.ImageLoader.bootstrap({}, document); }})();</script><script>Squarespace.afterBodyLoad(Y);</script>






<script>mendeleyWebImporter = {
  downloadPdfs(t) { return this._call('downloadPdfs', [t]); },
  open() { return this._call('open', []); },
  _call(methodName, methodArgs) {
    const id = Math.random();
    window.postMessage({ id, token: '0.9965609198607304', methodName, methodArgs }, 'http://www.figlab.com');
    return new Promise(resolve => {
      const listener = window.addEventListener('message', event => {
        const data = event.data;
        if (typeof data !== 'object' || !('result' in data) || data.id !== id) return;
        window.removeEventListener('message', listener);
        resolve(data.result);
      });
    });
  }
};</script></body></html>
